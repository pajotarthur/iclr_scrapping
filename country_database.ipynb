{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TL;DR</th>\n",
       "      <th>_bibtex</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authorids</th>\n",
       "      <th>authors</th>\n",
       "      <th>conf_1</th>\n",
       "      <th>conf_2</th>\n",
       "      <th>conf_3</th>\n",
       "      <th>decision</th>\n",
       "      <th>keywords</th>\n",
       "      <th>paperhash</th>\n",
       "      <th>pdf</th>\n",
       "      <th>review</th>\n",
       "      <th>review_1</th>\n",
       "      <th>review_2</th>\n",
       "      <th>review_3</th>\n",
       "      <th>title</th>\n",
       "      <th>withdrawal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Discover the structure of functional causal mo...</td>\n",
       "      <td>@article{\\ngoudet2018causal,\\ntitle={Causal Ge...</td>\n",
       "      <td>We introduce CGNN, a framework to learn functi...</td>\n",
       "      <td>[olivier.goudet@lri.fr, diviyan.kalainathan@lr...</td>\n",
       "      <td>[Olivier Goudet, Diviyan Kalainathan, David Lo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Causal structure discovery, Generative neural...</td>\n",
       "      <td>goudet|causal_generative_neural_networks</td>\n",
       "      <td>/pdf/653017947de7c5ba987bd7db5429b9239fbac2b1.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Causal Generative Neural Networks</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\nsharma2018hyperedge2vec:,\\ntitle={H...</td>\n",
       "      <td>Data structured in form of overlapping or non-...</td>\n",
       "      <td>[sharm170@umn.edu, srjoty@ntu.edu.sg, himanshu...</td>\n",
       "      <td>[Ankit Sharma, Shafiq Joty, Himanshu Kharkwal,...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[hypergraph, representation learning, tensors]</td>\n",
       "      <td>sharma|hyperedge2vec_distributed_representatio...</td>\n",
       "      <td>/pdf/df0bce76f679daed6584e6c2fa64e70dfeadcbb2.pdf</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hyperedge2vec: Distributed Representations for...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Query-based black-box attacks on deep neural n...</td>\n",
       "      <td>@article{\\nnitin2018exploring,\\ntitle={Explori...</td>\n",
       "      <td>Existing black-box attacks on deep neural netw...</td>\n",
       "      <td>[abhagoji@princeton.edu, _w@eecs.berkeley.edu,...</td>\n",
       "      <td>[Arjun Nitin Bhagoji, Warren He, Bo Li, Dawn S...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[adversarial machine learning, black-box attacks]</td>\n",
       "      <td>bhagoji|exploring_the_space_of_blackbox_attack...</td>\n",
       "      <td>/pdf/5f3238f70f31480b9668afbd0ece2e7adfe28480.pdf</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Exploring the Space of Black-box Attacks on De...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>A theory and algorithmic framework for predict...</td>\n",
       "      <td>@article{\\nd.2018learning,\\ntitle={Learning We...</td>\n",
       "      <td>Predictive models that generalize well under d...</td>\n",
       "      <td>[fredrikj@mit.edu, kallus@cornell.edu, urish22...</td>\n",
       "      <td>[Fredrik D. Johansson, Nathan Kallus, Uri Shal...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Distributional shift, causal effects, domain ...</td>\n",
       "      <td>johansson|learning_weighted_representations_fo...</td>\n",
       "      <td>/pdf/4f50fa235bd67edc0f4fe1575d4ca3aea418f368.pdf</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Learning Weighted Representations for Generali...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>We prove that DNN is a recursively approximate...</td>\n",
       "      <td>@article{\\nzheng2018understanding,\\ntitle={Und...</td>\n",
       "      <td>Deep learning achieves remarkable generalizati...</td>\n",
       "      <td>[zhenggh@mail.ustc.edu.cn, jtsang@bjtu.edu.cn,...</td>\n",
       "      <td>[Guanhua Zheng, Jitao Sang, Changsheng Xu]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[generalization, maximum entropy, deep learning]</td>\n",
       "      <td>zheng|understanding_deep_learning_generalizati...</td>\n",
       "      <td>/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Understanding Deep Learning Generalization by ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>This paper tries to preliminarily address the ...</td>\n",
       "      <td>@article{\\nliu2018preliminary,\\ntitle={Prelimi...</td>\n",
       "      <td>What would be learned by variational autoencod...</td>\n",
       "      <td>[liushiqi@stu.xjtu.edu.cn, dymeng@mail.xjtu.ed...</td>\n",
       "      <td>[Shiqi Liu, Qian Zhao, Xiangyong Cao, Deyu Meng]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[variational autoencoder, information theory, ...</td>\n",
       "      <td>liu|preliminary_theoretical_troubleshooting_in...</td>\n",
       "      <td>/pdf/c8ae63388eaa3fb6206aeb98a94bc8a3d0b921e9.pdf</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Preliminary theoretical troubleshooting in Var...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Unsupervised classification via deep generativ...</td>\n",
       "      <td>@article{\\nkalatzis2018towards,\\ntitle={Toward...</td>\n",
       "      <td>Deep generative models have advanced the state...</td>\n",
       "      <td>[dkal@iti.gr, ntina_kotta@yahoo.com, kalamar@i...</td>\n",
       "      <td>[Dimitris Kalatzis, Konstantia Kotta, Ilias Ka...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[variational inference, vae, variational autoe...</td>\n",
       "      <td>kalatzis|towards_unsupervised_classification_w...</td>\n",
       "      <td>/pdf/ff5681a1a9316f4ac3e53e4a01ccd192d4d6e187.pdf</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Towards Unsupervised Classification with Deep ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\nphuong2018the,\\ntitle={The Mutual A...</td>\n",
       "      <td>Variational autoencoders (VAE) learn probabili...</td>\n",
       "      <td>[bphuong@ist.ac.at, m.welling@uva.nl, nkushman...</td>\n",
       "      <td>[Mary Phuong, Max Welling, Nate Kushman, Ryota...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[]</td>\n",
       "      <td>phuong|the_mutual_autoencoder_controlling_info...</td>\n",
       "      <td>/pdf/0de2556a527a6a914b6205ba3128aef4a860889c.pdf</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The Mutual Autoencoder: Controlling Informatio...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\nhu2018learning,\\ntitle={Learning De...</td>\n",
       "      <td>There have been numerous recent advancements o...</td>\n",
       "      <td>[hengyuah@andrew.cmu.edu, rsalakhu@cs.cmu.edu]</td>\n",
       "      <td>[Hengyuan Hu, Ruslan Salakhutdinov]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[deep generative models, deep learning]</td>\n",
       "      <td>hu|learning_deep_generative_models_with_discre...</td>\n",
       "      <td>/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Learning Deep Generative Models With Discrete ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\nche2018combining,\\ntitle={Combining...</td>\n",
       "      <td>Model-free deep reinforcement learning algorit...</td>\n",
       "      <td>[gerryche@berkeley.edu, luyuchen.paul@gmail.co...</td>\n",
       "      <td>[Tong Che, Yuchen Lu, Chen Xing, Yoshua Bengio]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[]</td>\n",
       "      <td>che|combining_modelbased_and_modelfree_rl_via_...</td>\n",
       "      <td>/pdf/c94761f85f8bdbd8b9c53261e25b4ec0258406e8.pdf</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Combining Model-based and Model-free RL via Mu...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\ndurkan2018the,\\ntitle={The Context-...</td>\n",
       "      <td>One important aspect of generalization in mach...</td>\n",
       "      <td>[conor.durkan@ed.ac.uk, a.storkey@ed.ac.uk, h....</td>\n",
       "      <td>[Conor Durkan, Amos Storkey, Harrison Edwards]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[]</td>\n",
       "      <td>durkan|the_contextaware_learner</td>\n",
       "      <td>/pdf/012015f2dd7c7c2ddda23a8015bfb38251b06480.pdf</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The Context-Aware Learner</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>we present LSH Softmax, a softmax approximatio...</td>\n",
       "      <td>@article{\\nlevy2018lsh,\\ntitle={LSH Softmax: S...</td>\n",
       "      <td>Log-linear models models are widely used in ma...</td>\n",
       "      <td>[danilevy@cs.stanford.edu, taineleau@gmail.com...</td>\n",
       "      <td>[Daniel Levy, Danlu Chan, Stefano Ermon]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[LSH, softmax, deep, learning, sub, linear, ef...</td>\n",
       "      <td>levy|lsh_softmax_sublinear_learning_and_infere...</td>\n",
       "      <td>/pdf/75e4b4dff6a768550770d7c5879285034f53de0b.pdf</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LSH Softmax: Sub-Linear Learning and Inference...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\nblot2018shade:,\\ntitle={SHADE: SHAn...</td>\n",
       "      <td>Regularization is a big issue for training dee...</td>\n",
       "      <td>[michael.blot@lip6.fr, thomas.robert@lip6.fr, ...</td>\n",
       "      <td>[Michael Blot, Thomas Robert, Nicolas Thome, M...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[]</td>\n",
       "      <td>blot|shade_shannon_decay_informationbased_regu...</td>\n",
       "      <td>/pdf/4bef15180836618ae528217a23241f884cb4a5c7.pdf</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>SHADE: SHAnnon DEcay Information-Based Regular...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Simple generative approach to solve the word a...</td>\n",
       "      <td>@article{\\nkrishna2018a,\\ntitle={A closer look...</td>\n",
       "      <td>Although word analogy problems have become a s...</td>\n",
       "      <td>[siddharthkumar@upwork.com]</td>\n",
       "      <td>[Siddharth Krishna Kumar]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[word2vec, glove, word analogy, word relations...</td>\n",
       "      <td>kumar|a_closer_look_at_the_word_analogy_problem</td>\n",
       "      <td>/pdf/f89cff1f0a3406cc6753a9c3f2ab68f629fbbef3.pdf</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A closer look at the word analogy problem</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>A Bayesian Nonparametric Topic Model with Vari...</td>\n",
       "      <td>@article{\\nning2018a,\\ntitle={A Bayesian Nonpa...</td>\n",
       "      <td>Topic modeling of text documents is one of the...</td>\n",
       "      <td>[foxdoraame@gmail.com, yzheng3xg@gmail.com, zj...</td>\n",
       "      <td>[Xuefei Ning, Yin Zheng, Zhuxi Jiang, Yu Wang,...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[topic model, Bayesian nonparametric, variatio...</td>\n",
       "      <td>ning|a_bayesian_nonparametric_topic_model_with...</td>\n",
       "      <td>/pdf/1426c0861384d97ede770bc4277cbfb13be88535.pdf</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A Bayesian Nonparametric Topic Model with Vari...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\nlucas2018auxiliary,\\ntitle={Auxilia...</td>\n",
       "      <td>Generative modeling of high-dimensional data i...</td>\n",
       "      <td>[thomas.lucas@inria.fr, jakob.verbeek@inria.fr]</td>\n",
       "      <td>[Thomas Lucas, Jakob Verbeek]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[]</td>\n",
       "      <td>lucas|auxiliary_guided_autoregressive_variatio...</td>\n",
       "      <td>/pdf/593becc731d948de5bea1806259dbdd34790042f.pdf</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Auxiliary Guided Autoregressive Variational Au...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Proposed System can prevent impersonators with...</td>\n",
       "      <td>@article{\\nnandkishor2018application,\\ntitle={...</td>\n",
       "      <td>The paper proposes and demonstrates a Deep Con...</td>\n",
       "      <td>[kothawadesuraj@sggs.ac.in, tamgalesumit@sggs....</td>\n",
       "      <td>[Suraj Nandkishor Kothawade, Sumit Baburao Tam...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Deep Convolutional Neural Network, Disguised ...</td>\n",
       "      <td>kothawade|application_of_deep_convolutional_ne...</td>\n",
       "      <td>/pdf/9b88d7b69a2d508e386ef5ef42637cf56e79b387.pdf</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>APPLICATION OF DEEP CONVOLUTIONAL NEURAL NETWO...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>3 thrusts serving as stepping stones for robot...</td>\n",
       "      <td>@article{\\na2018towards,\\ntitle={TOWARDS ROBOT...</td>\n",
       "      <td>n this paper we present a thrust in three dire...</td>\n",
       "      <td>[aaa2cn@virginia.edu, jbd@virginia.edu]</td>\n",
       "      <td>[Ahmed A Aly, Joanne Bechta Dugan]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Deep Learning, Robotics, Artificial Intellige...</td>\n",
       "      <td>aly|towards_robot_vision_module_development_wi...</td>\n",
       "      <td>/pdf/00e5c4aefc80d0396ee745c032d27e0bccb43079.pdf</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH E...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Facing complex, black-box models, encrypting t...</td>\n",
       "      <td>@article{\\nxu2018don't,\\ntitle={Don't encrypt ...</td>\n",
       "      <td>As machine learning becomes ubiquitous, deploy...</td>\n",
       "      <td>[xxu@hmc.edu]</td>\n",
       "      <td>[Xinlei Xu]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Applications, Security in Machine Learning, F...</td>\n",
       "      <td>xu|dont_encrypt_the_data_just_approximate_the_...</td>\n",
       "      <td>/pdf/69170f53ffe9f431f2c54cd1a453add292d356cb.pdf</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Don't encrypt the data; just approximate the m...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Pros and cons of saccade-based computer vision...</td>\n",
       "      <td>@article{\\ndaucé2018toward,\\ntitle={Toward pre...</td>\n",
       "      <td>We develop a comprehensive description of the ...</td>\n",
       "      <td>[emmanuel.dauce@centrale-marseille.fr]</td>\n",
       "      <td>[Emmanuel Daucé]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[active inference, predictive coding, motor co...</td>\n",
       "      <td>daucé|toward_predictive_machine_learning_for_a...</td>\n",
       "      <td>/pdf/c860906df0b1e08d5e8c36481186f1820a29f58f.pdf</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Toward predictive machine learning for active ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Learning generative models for faces with real...</td>\n",
       "      <td>@article{\\npeterson2018learning,\\ntitle={Learn...</td>\n",
       "      <td>Generative models of human identity and appear...</td>\n",
       "      <td>[jpeterson@berkeley.edu, suchow@berkeley.edu, ...</td>\n",
       "      <td>[Joshua Peterson, Jordan Suchow, Thomas Griffi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[face perception, generative models, psychology]</td>\n",
       "      <td>peterson|learning_a_face_space_for_experiments...</td>\n",
       "      <td>/pdf/8338680582860322884e2d86319674906ab06dbd.pdf</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Learning a face space for experiments on human...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>We successfully convert a popular detector RPN...</td>\n",
       "      <td>@article{\\nyan2018tracking,\\ntitle={Tracking L...</td>\n",
       "      <td>In this paper, we find that by designing a nov...</td>\n",
       "      <td>[zhenb.yan@gmail.com, jimmy.sj.ren@gmail.com, ...</td>\n",
       "      <td>[Zhenbin Yan, Jimmy Ren, Stephen Shaoyi Liao, ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Object detection, Visual Tracking, Loss funct...</td>\n",
       "      <td>yan|tracking_loss_converting_object_detector_t...</td>\n",
       "      <td>/pdf/06ea09534e938f0792175ba1cb770b994107116e.pdf</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Tracking Loss: Converting Object Detector to R...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>We made a feature-rich system for deep learnin...</td>\n",
       "      <td>@article{\\nmeehan2018deep,\\ntitle={Deep Learni...</td>\n",
       "      <td>When deep learning is applied to sensitive dat...</td>\n",
       "      <td>[anthonymeehan@anthonymeehan.com, ryan.ko@waik...</td>\n",
       "      <td>[Anthony Meehan, Ryan K L Ko, Geoff Holmes]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[deep learning, homomorphic encryption, hybrid...</td>\n",
       "      <td>meehan|deep_learning_inferences_with_hybrid_ho...</td>\n",
       "      <td>/pdf/43dae54cad481416104f335c5fb25cd47ca4a89d.pdf</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Deep Learning Inferences with Hybrid Homomorph...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>In this paper, a new method we call Centered ...</td>\n",
       "      <td>@article{\\naddad2018clipping,\\ntitle={Clipping...</td>\n",
       "      <td>During the last years, a remarkable breakthrou...</td>\n",
       "      <td>[boussad.addad@thalesgroup.com, boussad83@yaho...</td>\n",
       "      <td>[Boussad ADDAD]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Adversarial examples, Neural Networks, Clipping]</td>\n",
       "      <td>addad|clipping_free_attacks_against_neural_net...</td>\n",
       "      <td>/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Clipping Free Attacks Against Neural Networks</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Generatively discover meaningful, novel entity...</td>\n",
       "      <td>@article{\\nzhang2018generative,\\ntitle={Genera...</td>\n",
       "      <td>Online healthcare services can provide the gen...</td>\n",
       "      <td>[czhang99@uic.edu, yaliangli@baidu.com, nandu@...</td>\n",
       "      <td>[Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, P...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Knowledge Discovery, Generative Modeling, Med...</td>\n",
       "      <td>zhang|generative_discovery_of_relational_medic...</td>\n",
       "      <td>/pdf/1b18496cb3dd605b82c84d2db7d3ac4387f7c56d.pdf</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Generative Discovery of Relational Medical Ent...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>first deep neural network for modeling Egocent...</td>\n",
       "      <td>@article{\\nzhang2018egocentric,\\ntitle={Egocen...</td>\n",
       "      <td>Inspired by neurophysiological discoveries of ...</td>\n",
       "      <td>[a0091624@u.nus.edu, makt@i2r.a-star.edu.sg, j...</td>\n",
       "      <td>[Mengmi Zhang, Keng Teck Ma, Joo Hwee Lim, Shi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[spatial memory, egocentric vision, deep neura...</td>\n",
       "      <td>zhang|egocentric_spatial_memory_network</td>\n",
       "      <td>/pdf/56fc3f6ef511eeca73d0c621cdc76a1ea80c40fe.pdf</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Egocentric Spatial Memory Network</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\ndinesh2018lsd-net:,\\ntitle={LSD-Net...</td>\n",
       "      <td>Multi-view recognition is the task of classify...</td>\n",
       "      <td>[dnarapur@andrew.cmu.edu]</td>\n",
       "      <td>[N dinesh reddy]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[]</td>\n",
       "      <td>reddy|lsdnet_look_step_and_detect_for_joint_na...</td>\n",
       "      <td>/pdf/723b94906b2a6cdd548a737caefe7362143054c9.pdf</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>LSD-Net: Look, Step and Detect for Joint Navig...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>combine reinforcement learning and imitation l...</td>\n",
       "      <td>@article{\\nzhu2018reinforcement,\\ntitle={Reinf...</td>\n",
       "      <td>We propose a general deep reinforcement learni...</td>\n",
       "      <td>[yukez@cs.stanford.edu, ziyu@google.com, jsmer...</td>\n",
       "      <td>[Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu,...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[reinforcement learning, imitation learning, r...</td>\n",
       "      <td>zhu|reinforcement_and_imitation_learning_for_d...</td>\n",
       "      <td>/pdf/c39982c4ecfdad60fa6d9794ddff7574bb1cb21d.pdf</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reinforcement and Imitation Learning for Diver...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Learn to convert a hand drawn sketch into a hi...</td>\n",
       "      <td>@article{\\nellis2018learning,\\ntitle={Learning...</td>\n",
       "      <td>We introduce a model that learns to convert ...</td>\n",
       "      <td>[ellisk@mit.edu, daniel_richie@brown.edu, asol...</td>\n",
       "      <td>[Kevin Ellis, Daniel Ritchie, Armando Solar-Le...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[program induction, HCI, deep learning]</td>\n",
       "      <td>ellis|learning_to_infer_graphics_programs_from...</td>\n",
       "      <td>/pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Learning to Infer Graphics Programs from Hand-...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>To enable cloud-based DNN training while prote...</td>\n",
       "      <td>@article{\\nli2018privynet:,\\ntitle={PrivyNet: ...</td>\n",
       "      <td>Massive data exist among user local platforms ...</td>\n",
       "      <td>[meng_li@utexas.edu, liangzhen.lai@arm.com, na...</td>\n",
       "      <td>[Meng Li, Liangzhen Lai, Naveen Suda, Vikas Ch...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Privacy-preserving deep learning, Neural netw...</td>\n",
       "      <td>li|privynet_a_flexible_framework_for_privacypr...</td>\n",
       "      <td>/pdf/49b8492b43883423a3e61b150d43dba83d968eea.pdf</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PrivyNet: A Flexible Framework for Privacy-Pre...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>We introduce contextual decompositions, an int...</td>\n",
       "      <td>@article{\\njames2018beyond,\\ntitle={Beyond Wor...</td>\n",
       "      <td>The driving force behind the recent success of...</td>\n",
       "      <td>[jmurdoch@berkeley.edu, peterjliu@google.com, ...</td>\n",
       "      <td>[W. James Murdoch, Peter J. Liu, Bin Yu]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[interpretability, LSTM, natural language proc...</td>\n",
       "      <td>murdoch|beyond_word_importance_contextual_deco...</td>\n",
       "      <td>/pdf/7ca2aa226aedab1064d25dab76f0f65e749a7b5f.pdf</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Beyond Word Importance:  Contextual Decomposit...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>Agents can learn to imitate solely visual demo...</td>\n",
       "      <td>@article{\\npathak2018zero-shot,\\ntitle={Zero-S...</td>\n",
       "      <td>Existing approaches to imitation learning dist...</td>\n",
       "      <td>[pathak@berkeley.edu, parsa.m@berkeley.edu, mi...</td>\n",
       "      <td>[Deepak Pathak*, Parsa Mahmoudieh*, Michael Lu...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[imitation, zero shot, self-supervised, roboti...</td>\n",
       "      <td>pathak|zeroshot_visual_imitation</td>\n",
       "      <td>/pdf/033006fc0917363d809a60477a753aecc800ddf0.pdf</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Zero-Shot Visual Imitation</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>Action-dependent baselines can be bias-free an...</td>\n",
       "      <td>@article{\\nwu2018variance,\\ntitle={Variance Re...</td>\n",
       "      <td>Policy gradient methods have enjoyed great suc...</td>\n",
       "      <td>[cathywu@eecs.berkeley.edu, aravraj@cs.washing...</td>\n",
       "      <td>[Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikas...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[reinforcement learning, policy gradient, vari...</td>\n",
       "      <td>wu|variance_reduction_for_policy_gradient_with...</td>\n",
       "      <td>/pdf/5c6eec975e91b7749a04d8daf58a692a38054869.pdf</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Variance Reduction for Policy Gradient with Ac...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>We train generative adversarial networks in a ...</td>\n",
       "      <td>@article{\\nkarras2018progressive,\\ntitle={Prog...</td>\n",
       "      <td>We describe a new training methodology for gen...</td>\n",
       "      <td>[tkarras@nvidia.com, taila@nvidia.com, slaine@...</td>\n",
       "      <td>[Tero Karras, Timo Aila, Samuli Laine, Jaakko ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[generative adversarial networks, unsupervised...</td>\n",
       "      <td>karras|progressive_growing_of_gans_for_improve...</td>\n",
       "      <td>/pdf/adb9f52be84187f86cf03bf5d69e13f27c9d2d2b.pdf</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Progressive Growing of GANs for Improved Quali...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>We give a method for generating type-safe prog...</td>\n",
       "      <td>@article{\\nmurali2018neural,\\ntitle={Neural Sk...</td>\n",
       "      <td>We study the problem of generating source code...</td>\n",
       "      <td>[vijay@rice.edu, letao.qi@rice.edu, swarat@ric...</td>\n",
       "      <td>[Vijayaraghavan Murali, Letao Qi, Swarat Chaud...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[Program generation, Source code, Program synt...</td>\n",
       "      <td>murali|neural_sketch_learning_for_conditional_...</td>\n",
       "      <td>/pdf/1ca8eeb12cfbc94b3a97d0a17d39cb5a896a4294.pdf</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Neural Sketch Learning for Conditional Program...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>We introduce the notion of mixed tensor decomp...</td>\n",
       "      <td>@article{\\ncohen2018boosting,\\ntitle={Boosting...</td>\n",
       "      <td>The driving force behind deep networks is thei...</td>\n",
       "      <td>[cohennadav@ias.edu, ronent@cs.huji.ac.il, sha...</td>\n",
       "      <td>[Nadav Cohen, Ronen Tamari, Amnon Shashua]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[Deep Learning, Expressive Efficiency, Dilated...</td>\n",
       "      <td>cohen|boosting_dilated_convolutional_networks_...</td>\n",
       "      <td>/pdf/71a73fa97f82c27d27f85d0ab4b65795c329ca33.pdf</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Boosting Dilated Convolutional Networks with M...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\nal-shedivat2018continuous,\\ntitle={...</td>\n",
       "      <td>Ability to continuously learn and adapt from l...</td>\n",
       "      <td>[alshedivat@cs.cmu.edu, tbansal@cs.umass.edu, ...</td>\n",
       "      <td>[Maruan Al-Shedivat, Trapit Bansal, Yura Burda...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[reinforcement learning, nonstationarity, meta...</td>\n",
       "      <td>alshedivat|continuous_adaptation_via_metalearn...</td>\n",
       "      <td>/pdf/14ddbf21a83f37283d0bc521de3d6bffaeca72dd.pdf</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Continuous Adaptation via Meta-Learning in Non...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>An approach to learning a shared embedding spa...</td>\n",
       "      <td>@article{\\ns.2018domain,\\ntitle={Domain Adapta...</td>\n",
       "      <td>Many deep reinforcement learning approaches us...</td>\n",
       "      <td>[d.ratcliffe@qmul.ac.uk, lciti@essex.ac.uk, sa...</td>\n",
       "      <td>[Dino S. Ratcliffe, Luca Citi, Sam Devlin, Udo...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Deep Reinforcement Learning, Domain Adaptatio...</td>\n",
       "      <td>ratcliffe|domain_adaptation_for_deep_reinforce...</td>\n",
       "      <td>/pdf/7e363a7fca433fde62ac5acc4d8fc8a55eff2a66.pdf</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Domain Adaptation for Deep Reinforcement Learn...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\nyang2018breaking,\\ntitle={Breaking ...</td>\n",
       "      <td>We formulate language modeling as a matrix fac...</td>\n",
       "      <td>[zhiliny@cs.cmu.edu, zander.dai@gmail.com, rsa...</td>\n",
       "      <td>[Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[]</td>\n",
       "      <td>yang|breaking_the_softmax_bottleneck_a_highran...</td>\n",
       "      <td>/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Breaking the Softmax Bottleneck: A High-Rank R...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>We characterize the dimensional properties of ...</td>\n",
       "      <td>@article{\\nma2018characterizing,\\ntitle={Chara...</td>\n",
       "      <td>Deep Neural Networks (DNNs) have recently been...</td>\n",
       "      <td>[xingjunm@student.unimelb.edu.au, crystalboli@...</td>\n",
       "      <td>[Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfan...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[Adversarial Subspace, Local Intrinsic Dimensi...</td>\n",
       "      <td>ma|characterizing_adversarial_subspaces_using_...</td>\n",
       "      <td>/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Characterizing Adversarial Subspaces Using Loc...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>Programs have structure that can be represente...</td>\n",
       "      <td>@article{\\nallamanis2018learning,\\ntitle={Lear...</td>\n",
       "      <td>Learning tasks on source code (i.e., formal la...</td>\n",
       "      <td>[t-mialla@microsoft.com, mabrocks@microsoft.co...</td>\n",
       "      <td>[Miltiadis Allamanis, Marc Brockschmidt, Mahmo...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[programs, source code, graph neural networks]</td>\n",
       "      <td>allamanis|learning_to_represent_programs_with_...</td>\n",
       "      <td>/pdf/87e7ee6e015ecdafc351607a7b52e8dc87b3bc7c.pdf</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Learning to Represent Programs with Graphs</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>We propose a novel weight normalization techni...</td>\n",
       "      <td>@article{\\nmiyato2018spectral,\\ntitle={Spectra...</td>\n",
       "      <td>One of the challenges in the study of generati...</td>\n",
       "      <td>[miyato@preferred.jp, kataoka@preferred.jp, ko...</td>\n",
       "      <td>[Takeru Miyato, Toshiki Kataoka, Masanori Koya...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[Generative Adversarial Networks, Deep Generat...</td>\n",
       "      <td>miyato|spectral_normalization_for_generative_a...</td>\n",
       "      <td>/pdf/c108c5657f8feb5338ed5bbb0ec6b430b0951880.pdf</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Spectral Normalization for Generative Adversar...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>We propose a new auto-encoder based on the Was...</td>\n",
       "      <td>@article{\\ntolstikhin2018wasserstein,\\ntitle={...</td>\n",
       "      <td>We propose the Wasserstein Auto-Encoder (WAE)-...</td>\n",
       "      <td>[iliya.tolstikhin@gmail.com, obousquet@gmail.c...</td>\n",
       "      <td>[Ilya Tolstikhin, Olivier Bousquet, Sylvain Ge...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[auto-encoder, generative models, GAN, VAE, un...</td>\n",
       "      <td>tolstikhin|wasserstein_autoencoders</td>\n",
       "      <td>/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Wasserstein Auto-Encoders</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Inference of a mean field game (MFG) model of ...</td>\n",
       "      <td>@article{\\nyang2018deep,\\ntitle={Deep Mean Fie...</td>\n",
       "      <td>We consider the problem of representing a larg...</td>\n",
       "      <td>[yjiachen@gmail.com, xye@gsu.edu, rstrivedi@ga...</td>\n",
       "      <td>[Jiachen Yang, Xiaojing Ye, Rakshit Trivedi, H...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[mean field games, reinforcement learning, Mar...</td>\n",
       "      <td>yang|deep_mean_field_games_for_learning_optima...</td>\n",
       "      <td>/pdf/8f628f760019758bad84d25565a929d7cc5f3d3a.pdf</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Deep Mean Field Games for Learning Optimal Beh...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>We provide a fast, principled adversarial trai...</td>\n",
       "      <td>@article{\\nsinha2018certifiable,\\ntitle={Certi...</td>\n",
       "      <td>Neural networks are vulnerable to adversaria...</td>\n",
       "      <td>[amans@stanford.edu, hnamk@stanford.edu, jduch...</td>\n",
       "      <td>[Aman Sinha, Hongseok Namkoong, John Duchi]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[adversarial training, distributionally robust...</td>\n",
       "      <td>sinha|certifiable_distributional_robustness_wi...</td>\n",
       "      <td>/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Certifiable Distributional Robustness with Pri...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>Existing momentum/acceleration schemes such as...</td>\n",
       "      <td>@article{\\nkidambi2018on,\\ntitle={On the insuf...</td>\n",
       "      <td>Momentum based stochastic gradient methods suc...</td>\n",
       "      <td>[rkidambi@uw.edu, praneeth@microsoft.com, praj...</td>\n",
       "      <td>[Rahul Kidambi, Praneeth Netrapalli, Prateek J...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[Stochastic Gradient Descent, Deep Learning, M...</td>\n",
       "      <td>kidambi|on_the_insufficiency_of_existing_momen...</td>\n",
       "      <td>/pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>On the insufficiency of existing momentum sche...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>We propose an agent that sits between the user...</td>\n",
       "      <td>@article{\\nbuck2018ask,\\ntitle={Ask the Right ...</td>\n",
       "      <td>We frame Question Answering as a Reinforcement...</td>\n",
       "      <td>[cbuck@google.com, jbulian@google.com, massi@g...</td>\n",
       "      <td>[Christian Buck, Jannis Bulian, Massimiliano C...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[machine translation, paraphrasing, question a...</td>\n",
       "      <td>buck|ask_the_right_questions_active_question_r...</td>\n",
       "      <td>/pdf/7dfeb1f2b789a55669bbf0487c07c7bde1b47ce1.pdf</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Ask the Right Questions: Active Question Refor...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>We introduce Spherical CNNs, a convolutional n...</td>\n",
       "      <td>@article{\\ns.2018spherical,\\ntitle={Spherical ...</td>\n",
       "      <td>Convolutional Neural Networks (CNNs) have beco...</td>\n",
       "      <td>[taco.cohen@gmail.com, geiger.mario@gmail.com,...</td>\n",
       "      <td>[Taco S. Cohen, Mario Geiger, Jonas Köhler, Ma...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[deep learning, equivariance, convolution, gro...</td>\n",
       "      <td>cohen|spherical_cnns</td>\n",
       "      <td>/pdf/e8528121f5dc4d01298ed84ad29be7b64483d9b7.pdf</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Spherical CNNs</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>learning better domain embeddings via lifelong...</td>\n",
       "      <td>@article{\\nxu2018lifelong,\\ntitle={Lifelong Wo...</td>\n",
       "      <td>Learning high-quality word embeddings is of si...</td>\n",
       "      <td>[hxu48@uic.edu, liub@uic.edu, lshu3@uic.edu, p...</td>\n",
       "      <td>[Hu Xu, Bing Liu, Lei Shu, Philip S. Yu]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Lifelong learning, meta learning, word embedd...</td>\n",
       "      <td>xu|lifelong_word_embedding_via_metalearning</td>\n",
       "      <td>/pdf/2d129dcd1101fe5b2fbb27768913ebad82c5727e.pdf</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Lifelong Word Embedding via Meta-Learning</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>A controlled study of the role of environments...</td>\n",
       "      <td>@article{\\nlazaridou2018emergence,\\ntitle={Eme...</td>\n",
       "      <td>The ability of algorithms to evolve or learn (...</td>\n",
       "      <td>[angeliki@google.com, kmh@google.com, karltuyl...</td>\n",
       "      <td>[Angeliki Lazaridou, Karl Moritz Hermann, Karl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[disentanglement, communication, emergent lang...</td>\n",
       "      <td>lazaridou|emergence_of_linguistic_communicatio...</td>\n",
       "      <td>/pdf/03978df8e51cdd7d42821961e9cd5580a4596408.pdf</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Emergence of Linguistic Communication from  Re...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>We apply training and inference with only low-...</td>\n",
       "      <td>@article{\\nwu2018training,\\ntitle={Training an...</td>\n",
       "      <td>Researches on deep neural networks with discre...</td>\n",
       "      <td>[wus15@mails.tsinghua.edu.cn]</td>\n",
       "      <td>[Shuang Wu, Guoqi Li, Feng Chen, Luping Shi]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[quantization, training, bitwidth, ternary wei...</td>\n",
       "      <td>wu|training_and_inference_with_integers_in_dee...</td>\n",
       "      <td>/pdf/07987f73b6973548325e92cd622d18372790e45d.pdf</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Training and Inference with Integers in Deep N...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\nhuang2018multi-scale,\\ntitle={Multi...</td>\n",
       "      <td>In this paper we investigate image classificat...</td>\n",
       "      <td>[gh349@cornell.edu, taineleau@gmail.com, lth14...</td>\n",
       "      <td>[Gao Huang, Danlu Chen, Tianhong Li, Felix Wu,...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[efficient learning, budgeted learning, deep l...</td>\n",
       "      <td>huang|multiscale_dense_networks_for_resource_e...</td>\n",
       "      <td>/pdf/4794d433da3a530b573404aa3995beef700360d0.pdf</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Multi-Scale Dense Networks for Resource Effici...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>CharNMT is brittle</td>\n",
       "      <td>@article{\\nbelinkov2018synthetic,\\ntitle={Synt...</td>\n",
       "      <td>Character-based neural machine translation (NM...</td>\n",
       "      <td>[belinkov@mit.edu, ybisk@yonatanbisk.com]</td>\n",
       "      <td>[Yonatan Belinkov, Yonatan Bisk]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[neural machine translation, characters, noise...</td>\n",
       "      <td>belinkov|synthetic_and_natural_noise_both_brea...</td>\n",
       "      <td>/pdf/119672f6a6a0b3fe32fdf19ddee2a270ddebfd20.pdf</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Synthetic and Natural Noise Both Break Neural ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>We investigate the convergence of popular opti...</td>\n",
       "      <td>@article{\\nj.2018on,\\ntitle={On the Convergenc...</td>\n",
       "      <td>Several recently proposed stochastic optimiza...</td>\n",
       "      <td>[sashank@google.com, satyenkale@google.com, sa...</td>\n",
       "      <td>[Sashank J. Reddi, Satyen Kale, Sanjiv Kumar]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>[optimization, deep learning, adam, rmsprop]</td>\n",
       "      <td>reddi|on_the_convergence_of_adam_and_beyond</td>\n",
       "      <td>/pdf/f5e90bccde3ca55853c7482ec4c77b41aacc10e7.pdf</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>On the Convergence of Adam and Beyond</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>We introduce meta-adversarial learning, a new ...</td>\n",
       "      <td>@article{\\ngrewal2018variance,\\ntitle={Varianc...</td>\n",
       "      <td>We study how, in generative adversarial networ...</td>\n",
       "      <td>[karanraj.grewal@mail.utoronto.ca, erroneus@gm...</td>\n",
       "      <td>[Karan Grewal, R Devon Hjelm, Yoshua Bengio]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Generative Adversarial Network, Integral Prob...</td>\n",
       "      <td>grewal|variance_regularizing_adversarial_learning</td>\n",
       "      <td>/pdf/40314ec625815016cf5f00ff379932bcc1815bb8.pdf</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Variance Regularizing Adversarial Learning</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>This paper introduces a probabilistic framewor...</td>\n",
       "      <td>@article{\\nbauer2018discriminative,\\ntitle={Di...</td>\n",
       "      <td>This paper introduces a probabilistic framewor...</td>\n",
       "      <td>[msb55@cam.ac.uk, mrojascarulla@gmail.com, kub...</td>\n",
       "      <td>[Matthias Bauer, Mateo Rojas-Carulla, Jakub Ba...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[discriminative k-shot learning, probabilistic...</td>\n",
       "      <td>bauer|discriminative_kshot_learning_using_prob...</td>\n",
       "      <td>/pdf/c98e564bd99e7a5e1ca2dec57f97dfd703ee986f.pdf</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Discriminative k-shot learning using probabili...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>An alternative to transfer learning that learn...</td>\n",
       "      <td>@article{\\nrosenfeld2018incremental,\\ntitle={I...</td>\n",
       "      <td>Given an existing trained neural network, it i...</td>\n",
       "      <td>[amir.rosenfeld@gmail.com]</td>\n",
       "      <td>[Amir Rosenfeld, John K. Tsotsos]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Transfer Learning, Learning without forgettin...</td>\n",
       "      <td>rosenfeld|incremental_learning_through_deep_ad...</td>\n",
       "      <td>/pdf/85a6012f830b872e7a9e70c88a18f36383755bcd.pdf</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Incremental Learning through Deep Adaptation</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>We efficiently solve multi-task problems with ...</td>\n",
       "      <td>@article{\\nheld2018automatic,\\ntitle={Automati...</td>\n",
       "      <td>Reinforcement learning (RL) is a powerful tech...</td>\n",
       "      <td>[dheld@andrew.cmu.edu, young.geng@berkeley.edu...</td>\n",
       "      <td>[David Held, Xinyang Geng, Carlos Florensa, Pi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Reinforcement Learning, Multi-task Learning, ...</td>\n",
       "      <td>held|automatic_goal_generation_for_reinforceme...</td>\n",
       "      <td>/pdf/d3bea8d42d0595a7bc8e7dc4edf32c95e8f6a035.pdf</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Automatic Goal Generation for Reinforcement Le...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>An unsupervised domain adaptation approach whi...</td>\n",
       "      <td>@article{\\nhoffman2018cycada:,\\ntitle={CyCADA:...</td>\n",
       "      <td>Domain adaptation is critical for success in n...</td>\n",
       "      <td>[jhoffman@eecs.berkeley.edu, etzeng@eecs.berke...</td>\n",
       "      <td>[Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Y...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[domain adaptation, unsupervised learning, cla...</td>\n",
       "      <td>hoffman|cycada_cycleconsistent_adversarial_dom...</td>\n",
       "      <td>/pdf/45af0aaa43fe19af0d9e0eafa60d9252bbe3a60a.pdf</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>CyCADA: Cycle-Consistent Adversarial Domain Ad...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>None</td>\n",
       "      <td>@article{\\npang2018meta-learning,\\ntitle={Meta...</td>\n",
       "      <td>Active learning (AL) aims to enable training h...</td>\n",
       "      <td>[k.pang@ed.ac.uk, mingzhi.dong.13@ucl.ac.uk, t...</td>\n",
       "      <td>[Kunkun Pang, Mingzhi Dong, Timothy Hospedales]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Reject</td>\n",
       "      <td>[Active Learning, Deep Reinforcement Learning]</td>\n",
       "      <td>pang|metalearning_transferable_active_learning...</td>\n",
       "      <td>/pdf/667b2dc6585b9e6f09bfc409b5558469556484ba.pdf</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Meta-Learning Transferable Active Learning Pol...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 TL;DR  \\\n",
       "0    Discover the structure of functional causal mo...   \n",
       "1                                                 None   \n",
       "10   Query-based black-box attacks on deep neural n...   \n",
       "100  A theory and algorithmic framework for predict...   \n",
       "101  We prove that DNN is a recursively approximate...   \n",
       "102  This paper tries to preliminarily address the ...   \n",
       "103  Unsupervised classification via deep generativ...   \n",
       "104                                               None   \n",
       "105                                               None   \n",
       "106                                               None   \n",
       "107                                               None   \n",
       "108  we present LSH Softmax, a softmax approximatio...   \n",
       "109                                               None   \n",
       "11   Simple generative approach to solve the word a...   \n",
       "110  A Bayesian Nonparametric Topic Model with Vari...   \n",
       "111                                               None   \n",
       "112  Proposed System can prevent impersonators with...   \n",
       "113  3 thrusts serving as stepping stones for robot...   \n",
       "114  Facing complex, black-box models, encrypting t...   \n",
       "115  Pros and cons of saccade-based computer vision...   \n",
       "116  Learning generative models for faces with real...   \n",
       "117  We successfully convert a popular detector RPN...   \n",
       "118  We made a feature-rich system for deep learnin...   \n",
       "119   In this paper, a new method we call Centered ...   \n",
       "12   Generatively discover meaningful, novel entity...   \n",
       "120  first deep neural network for modeling Egocent...   \n",
       "121                                               None   \n",
       "122  combine reinforcement learning and imitation l...   \n",
       "123  Learn to convert a hand drawn sketch into a hi...   \n",
       "124  To enable cloud-based DNN training while prote...   \n",
       "..                                                 ...   \n",
       "913  We introduce contextual decompositions, an int...   \n",
       "914  Agents can learn to imitate solely visual demo...   \n",
       "915  Action-dependent baselines can be bias-free an...   \n",
       "916  We train generative adversarial networks in a ...   \n",
       "917  We give a method for generating type-safe prog...   \n",
       "918  We introduce the notion of mixed tensor decomp...   \n",
       "919                                               None   \n",
       "92   An approach to learning a shared embedding spa...   \n",
       "920                                               None   \n",
       "921  We characterize the dimensional properties of ...   \n",
       "922  Programs have structure that can be represente...   \n",
       "923  We propose a novel weight normalization techni...   \n",
       "924  We propose a new auto-encoder based on the Was...   \n",
       "925  Inference of a mean field game (MFG) model of ...   \n",
       "926  We provide a fast, principled adversarial trai...   \n",
       "927  Existing momentum/acceleration schemes such as...   \n",
       "928  We propose an agent that sits between the user...   \n",
       "929  We introduce Spherical CNNs, a convolutional n...   \n",
       "93   learning better domain embeddings via lifelong...   \n",
       "930  A controlled study of the role of environments...   \n",
       "931  We apply training and inference with only low-...   \n",
       "932                                               None   \n",
       "933                                 CharNMT is brittle   \n",
       "934  We investigate the convergence of popular opti...   \n",
       "94   We introduce meta-adversarial learning, a new ...   \n",
       "95   This paper introduces a probabilistic framewor...   \n",
       "96   An alternative to transfer learning that learn...   \n",
       "97   We efficiently solve multi-task problems with ...   \n",
       "98   An unsupervised domain adaptation approach whi...   \n",
       "99                                                None   \n",
       "\n",
       "                                               _bibtex  \\\n",
       "0    @article{\\ngoudet2018causal,\\ntitle={Causal Ge...   \n",
       "1    @article{\\nsharma2018hyperedge2vec:,\\ntitle={H...   \n",
       "10   @article{\\nnitin2018exploring,\\ntitle={Explori...   \n",
       "100  @article{\\nd.2018learning,\\ntitle={Learning We...   \n",
       "101  @article{\\nzheng2018understanding,\\ntitle={Und...   \n",
       "102  @article{\\nliu2018preliminary,\\ntitle={Prelimi...   \n",
       "103  @article{\\nkalatzis2018towards,\\ntitle={Toward...   \n",
       "104  @article{\\nphuong2018the,\\ntitle={The Mutual A...   \n",
       "105  @article{\\nhu2018learning,\\ntitle={Learning De...   \n",
       "106  @article{\\nche2018combining,\\ntitle={Combining...   \n",
       "107  @article{\\ndurkan2018the,\\ntitle={The Context-...   \n",
       "108  @article{\\nlevy2018lsh,\\ntitle={LSH Softmax: S...   \n",
       "109  @article{\\nblot2018shade:,\\ntitle={SHADE: SHAn...   \n",
       "11   @article{\\nkrishna2018a,\\ntitle={A closer look...   \n",
       "110  @article{\\nning2018a,\\ntitle={A Bayesian Nonpa...   \n",
       "111  @article{\\nlucas2018auxiliary,\\ntitle={Auxilia...   \n",
       "112  @article{\\nnandkishor2018application,\\ntitle={...   \n",
       "113  @article{\\na2018towards,\\ntitle={TOWARDS ROBOT...   \n",
       "114  @article{\\nxu2018don't,\\ntitle={Don't encrypt ...   \n",
       "115  @article{\\ndaucé2018toward,\\ntitle={Toward pre...   \n",
       "116  @article{\\npeterson2018learning,\\ntitle={Learn...   \n",
       "117  @article{\\nyan2018tracking,\\ntitle={Tracking L...   \n",
       "118  @article{\\nmeehan2018deep,\\ntitle={Deep Learni...   \n",
       "119  @article{\\naddad2018clipping,\\ntitle={Clipping...   \n",
       "12   @article{\\nzhang2018generative,\\ntitle={Genera...   \n",
       "120  @article{\\nzhang2018egocentric,\\ntitle={Egocen...   \n",
       "121  @article{\\ndinesh2018lsd-net:,\\ntitle={LSD-Net...   \n",
       "122  @article{\\nzhu2018reinforcement,\\ntitle={Reinf...   \n",
       "123  @article{\\nellis2018learning,\\ntitle={Learning...   \n",
       "124  @article{\\nli2018privynet:,\\ntitle={PrivyNet: ...   \n",
       "..                                                 ...   \n",
       "913  @article{\\njames2018beyond,\\ntitle={Beyond Wor...   \n",
       "914  @article{\\npathak2018zero-shot,\\ntitle={Zero-S...   \n",
       "915  @article{\\nwu2018variance,\\ntitle={Variance Re...   \n",
       "916  @article{\\nkarras2018progressive,\\ntitle={Prog...   \n",
       "917  @article{\\nmurali2018neural,\\ntitle={Neural Sk...   \n",
       "918  @article{\\ncohen2018boosting,\\ntitle={Boosting...   \n",
       "919  @article{\\nal-shedivat2018continuous,\\ntitle={...   \n",
       "92   @article{\\ns.2018domain,\\ntitle={Domain Adapta...   \n",
       "920  @article{\\nyang2018breaking,\\ntitle={Breaking ...   \n",
       "921  @article{\\nma2018characterizing,\\ntitle={Chara...   \n",
       "922  @article{\\nallamanis2018learning,\\ntitle={Lear...   \n",
       "923  @article{\\nmiyato2018spectral,\\ntitle={Spectra...   \n",
       "924  @article{\\ntolstikhin2018wasserstein,\\ntitle={...   \n",
       "925  @article{\\nyang2018deep,\\ntitle={Deep Mean Fie...   \n",
       "926  @article{\\nsinha2018certifiable,\\ntitle={Certi...   \n",
       "927  @article{\\nkidambi2018on,\\ntitle={On the insuf...   \n",
       "928  @article{\\nbuck2018ask,\\ntitle={Ask the Right ...   \n",
       "929  @article{\\ns.2018spherical,\\ntitle={Spherical ...   \n",
       "93   @article{\\nxu2018lifelong,\\ntitle={Lifelong Wo...   \n",
       "930  @article{\\nlazaridou2018emergence,\\ntitle={Eme...   \n",
       "931  @article{\\nwu2018training,\\ntitle={Training an...   \n",
       "932  @article{\\nhuang2018multi-scale,\\ntitle={Multi...   \n",
       "933  @article{\\nbelinkov2018synthetic,\\ntitle={Synt...   \n",
       "934  @article{\\nj.2018on,\\ntitle={On the Convergenc...   \n",
       "94   @article{\\ngrewal2018variance,\\ntitle={Varianc...   \n",
       "95   @article{\\nbauer2018discriminative,\\ntitle={Di...   \n",
       "96   @article{\\nrosenfeld2018incremental,\\ntitle={I...   \n",
       "97   @article{\\nheld2018automatic,\\ntitle={Automati...   \n",
       "98   @article{\\nhoffman2018cycada:,\\ntitle={CyCADA:...   \n",
       "99   @article{\\npang2018meta-learning,\\ntitle={Meta...   \n",
       "\n",
       "                                              abstract  \\\n",
       "0    We introduce CGNN, a framework to learn functi...   \n",
       "1    Data structured in form of overlapping or non-...   \n",
       "10   Existing black-box attacks on deep neural netw...   \n",
       "100  Predictive models that generalize well under d...   \n",
       "101  Deep learning achieves remarkable generalizati...   \n",
       "102  What would be learned by variational autoencod...   \n",
       "103  Deep generative models have advanced the state...   \n",
       "104  Variational autoencoders (VAE) learn probabili...   \n",
       "105  There have been numerous recent advancements o...   \n",
       "106  Model-free deep reinforcement learning algorit...   \n",
       "107  One important aspect of generalization in mach...   \n",
       "108  Log-linear models models are widely used in ma...   \n",
       "109  Regularization is a big issue for training dee...   \n",
       "11   Although word analogy problems have become a s...   \n",
       "110  Topic modeling of text documents is one of the...   \n",
       "111  Generative modeling of high-dimensional data i...   \n",
       "112  The paper proposes and demonstrates a Deep Con...   \n",
       "113  n this paper we present a thrust in three dire...   \n",
       "114  As machine learning becomes ubiquitous, deploy...   \n",
       "115  We develop a comprehensive description of the ...   \n",
       "116  Generative models of human identity and appear...   \n",
       "117  In this paper, we find that by designing a nov...   \n",
       "118  When deep learning is applied to sensitive dat...   \n",
       "119  During the last years, a remarkable breakthrou...   \n",
       "12   Online healthcare services can provide the gen...   \n",
       "120  Inspired by neurophysiological discoveries of ...   \n",
       "121  Multi-view recognition is the task of classify...   \n",
       "122  We propose a general deep reinforcement learni...   \n",
       "123    We introduce a model that learns to convert ...   \n",
       "124  Massive data exist among user local platforms ...   \n",
       "..                                                 ...   \n",
       "913  The driving force behind the recent success of...   \n",
       "914  Existing approaches to imitation learning dist...   \n",
       "915  Policy gradient methods have enjoyed great suc...   \n",
       "916  We describe a new training methodology for gen...   \n",
       "917  We study the problem of generating source code...   \n",
       "918  The driving force behind deep networks is thei...   \n",
       "919  Ability to continuously learn and adapt from l...   \n",
       "92   Many deep reinforcement learning approaches us...   \n",
       "920  We formulate language modeling as a matrix fac...   \n",
       "921  Deep Neural Networks (DNNs) have recently been...   \n",
       "922  Learning tasks on source code (i.e., formal la...   \n",
       "923  One of the challenges in the study of generati...   \n",
       "924  We propose the Wasserstein Auto-Encoder (WAE)-...   \n",
       "925  We consider the problem of representing a larg...   \n",
       "926    Neural networks are vulnerable to adversaria...   \n",
       "927  Momentum based stochastic gradient methods suc...   \n",
       "928  We frame Question Answering as a Reinforcement...   \n",
       "929  Convolutional Neural Networks (CNNs) have beco...   \n",
       "93   Learning high-quality word embeddings is of si...   \n",
       "930  The ability of algorithms to evolve or learn (...   \n",
       "931  Researches on deep neural networks with discre...   \n",
       "932  In this paper we investigate image classificat...   \n",
       "933  Character-based neural machine translation (NM...   \n",
       "934   Several recently proposed stochastic optimiza...   \n",
       "94   We study how, in generative adversarial networ...   \n",
       "95   This paper introduces a probabilistic framewor...   \n",
       "96   Given an existing trained neural network, it i...   \n",
       "97   Reinforcement learning (RL) is a powerful tech...   \n",
       "98   Domain adaptation is critical for success in n...   \n",
       "99   Active learning (AL) aims to enable training h...   \n",
       "\n",
       "                                             authorids  \\\n",
       "0    [olivier.goudet@lri.fr, diviyan.kalainathan@lr...   \n",
       "1    [sharm170@umn.edu, srjoty@ntu.edu.sg, himanshu...   \n",
       "10   [abhagoji@princeton.edu, _w@eecs.berkeley.edu,...   \n",
       "100  [fredrikj@mit.edu, kallus@cornell.edu, urish22...   \n",
       "101  [zhenggh@mail.ustc.edu.cn, jtsang@bjtu.edu.cn,...   \n",
       "102  [liushiqi@stu.xjtu.edu.cn, dymeng@mail.xjtu.ed...   \n",
       "103  [dkal@iti.gr, ntina_kotta@yahoo.com, kalamar@i...   \n",
       "104  [bphuong@ist.ac.at, m.welling@uva.nl, nkushman...   \n",
       "105     [hengyuah@andrew.cmu.edu, rsalakhu@cs.cmu.edu]   \n",
       "106  [gerryche@berkeley.edu, luyuchen.paul@gmail.co...   \n",
       "107  [conor.durkan@ed.ac.uk, a.storkey@ed.ac.uk, h....   \n",
       "108  [danilevy@cs.stanford.edu, taineleau@gmail.com...   \n",
       "109  [michael.blot@lip6.fr, thomas.robert@lip6.fr, ...   \n",
       "11                         [siddharthkumar@upwork.com]   \n",
       "110  [foxdoraame@gmail.com, yzheng3xg@gmail.com, zj...   \n",
       "111    [thomas.lucas@inria.fr, jakob.verbeek@inria.fr]   \n",
       "112  [kothawadesuraj@sggs.ac.in, tamgalesumit@sggs....   \n",
       "113            [aaa2cn@virginia.edu, jbd@virginia.edu]   \n",
       "114                                      [xxu@hmc.edu]   \n",
       "115             [emmanuel.dauce@centrale-marseille.fr]   \n",
       "116  [jpeterson@berkeley.edu, suchow@berkeley.edu, ...   \n",
       "117  [zhenb.yan@gmail.com, jimmy.sj.ren@gmail.com, ...   \n",
       "118  [anthonymeehan@anthonymeehan.com, ryan.ko@waik...   \n",
       "119  [boussad.addad@thalesgroup.com, boussad83@yaho...   \n",
       "12   [czhang99@uic.edu, yaliangli@baidu.com, nandu@...   \n",
       "120  [a0091624@u.nus.edu, makt@i2r.a-star.edu.sg, j...   \n",
       "121                          [dnarapur@andrew.cmu.edu]   \n",
       "122  [yukez@cs.stanford.edu, ziyu@google.com, jsmer...   \n",
       "123  [ellisk@mit.edu, daniel_richie@brown.edu, asol...   \n",
       "124  [meng_li@utexas.edu, liangzhen.lai@arm.com, na...   \n",
       "..                                                 ...   \n",
       "913  [jmurdoch@berkeley.edu, peterjliu@google.com, ...   \n",
       "914  [pathak@berkeley.edu, parsa.m@berkeley.edu, mi...   \n",
       "915  [cathywu@eecs.berkeley.edu, aravraj@cs.washing...   \n",
       "916  [tkarras@nvidia.com, taila@nvidia.com, slaine@...   \n",
       "917  [vijay@rice.edu, letao.qi@rice.edu, swarat@ric...   \n",
       "918  [cohennadav@ias.edu, ronent@cs.huji.ac.il, sha...   \n",
       "919  [alshedivat@cs.cmu.edu, tbansal@cs.umass.edu, ...   \n",
       "92   [d.ratcliffe@qmul.ac.uk, lciti@essex.ac.uk, sa...   \n",
       "920  [zhiliny@cs.cmu.edu, zander.dai@gmail.com, rsa...   \n",
       "921  [xingjunm@student.unimelb.edu.au, crystalboli@...   \n",
       "922  [t-mialla@microsoft.com, mabrocks@microsoft.co...   \n",
       "923  [miyato@preferred.jp, kataoka@preferred.jp, ko...   \n",
       "924  [iliya.tolstikhin@gmail.com, obousquet@gmail.c...   \n",
       "925  [yjiachen@gmail.com, xye@gsu.edu, rstrivedi@ga...   \n",
       "926  [amans@stanford.edu, hnamk@stanford.edu, jduch...   \n",
       "927  [rkidambi@uw.edu, praneeth@microsoft.com, praj...   \n",
       "928  [cbuck@google.com, jbulian@google.com, massi@g...   \n",
       "929  [taco.cohen@gmail.com, geiger.mario@gmail.com,...   \n",
       "93   [hxu48@uic.edu, liub@uic.edu, lshu3@uic.edu, p...   \n",
       "930  [angeliki@google.com, kmh@google.com, karltuyl...   \n",
       "931                      [wus15@mails.tsinghua.edu.cn]   \n",
       "932  [gh349@cornell.edu, taineleau@gmail.com, lth14...   \n",
       "933          [belinkov@mit.edu, ybisk@yonatanbisk.com]   \n",
       "934  [sashank@google.com, satyenkale@google.com, sa...   \n",
       "94   [karanraj.grewal@mail.utoronto.ca, erroneus@gm...   \n",
       "95   [msb55@cam.ac.uk, mrojascarulla@gmail.com, kub...   \n",
       "96                          [amir.rosenfeld@gmail.com]   \n",
       "97   [dheld@andrew.cmu.edu, young.geng@berkeley.edu...   \n",
       "98   [jhoffman@eecs.berkeley.edu, etzeng@eecs.berke...   \n",
       "99   [k.pang@ed.ac.uk, mingzhi.dong.13@ucl.ac.uk, t...   \n",
       "\n",
       "                                               authors  conf_1  conf_2  \\\n",
       "0    [Olivier Goudet, Diviyan Kalainathan, David Lo...     NaN     NaN   \n",
       "1    [Ankit Sharma, Shafiq Joty, Himanshu Kharkwal,...     3.0     3.0   \n",
       "10   [Arjun Nitin Bhagoji, Warren He, Bo Li, Dawn S...     4.0     3.0   \n",
       "100  [Fredrik D. Johansson, Nathan Kallus, Uri Shal...     3.0     3.0   \n",
       "101         [Guanhua Zheng, Jitao Sang, Changsheng Xu]     3.0     3.0   \n",
       "102   [Shiqi Liu, Qian Zhao, Xiangyong Cao, Deyu Meng]     4.0     4.0   \n",
       "103  [Dimitris Kalatzis, Konstantia Kotta, Ilias Ka...     4.0     4.0   \n",
       "104  [Mary Phuong, Max Welling, Nate Kushman, Ryota...     5.0     4.0   \n",
       "105                [Hengyuan Hu, Ruslan Salakhutdinov]     4.0     4.0   \n",
       "106    [Tong Che, Yuchen Lu, Chen Xing, Yoshua Bengio]     4.0     4.0   \n",
       "107     [Conor Durkan, Amos Storkey, Harrison Edwards]     5.0     3.0   \n",
       "108           [Daniel Levy, Danlu Chan, Stefano Ermon]     4.0     3.0   \n",
       "109  [Michael Blot, Thomas Robert, Nicolas Thome, M...     4.0     3.0   \n",
       "11                           [Siddharth Krishna Kumar]     5.0     4.0   \n",
       "110  [Xuefei Ning, Yin Zheng, Zhuxi Jiang, Yu Wang,...     4.0     4.0   \n",
       "111                      [Thomas Lucas, Jakob Verbeek]     4.0     4.0   \n",
       "112  [Suraj Nandkishor Kothawade, Sumit Baburao Tam...     5.0     4.0   \n",
       "113                 [Ahmed A Aly, Joanne Bechta Dugan]     4.0     3.0   \n",
       "114                                        [Xinlei Xu]     4.0     5.0   \n",
       "115                                   [Emmanuel Daucé]     4.0     2.0   \n",
       "116  [Joshua Peterson, Jordan Suchow, Thomas Griffi...     5.0     4.0   \n",
       "117  [Zhenbin Yan, Jimmy Ren, Stephen Shaoyi Liao, ...     4.0     5.0   \n",
       "118        [Anthony Meehan, Ryan K L Ko, Geoff Holmes]     5.0     5.0   \n",
       "119                                    [Boussad ADDAD]     3.0     3.0   \n",
       "12   [Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, P...     3.0     4.0   \n",
       "120  [Mengmi Zhang, Keng Teck Ma, Joo Hwee Lim, Shi...     4.0     4.0   \n",
       "121                                   [N dinesh reddy]     4.0     4.0   \n",
       "122  [Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu,...     5.0     4.0   \n",
       "123  [Kevin Ellis, Daniel Ritchie, Armando Solar-Le...     4.0     4.0   \n",
       "124  [Meng Li, Liangzhen Lai, Naveen Suda, Vikas Ch...     5.0     3.0   \n",
       "..                                                 ...     ...     ...   \n",
       "913           [W. James Murdoch, Peter J. Liu, Bin Yu]     4.0     2.0   \n",
       "914  [Deepak Pathak*, Parsa Mahmoudieh*, Michael Lu...     4.0     3.0   \n",
       "915  [Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikas...     4.0     3.0   \n",
       "916  [Tero Karras, Timo Aila, Samuli Laine, Jaakko ...     4.0     4.0   \n",
       "917  [Vijayaraghavan Murali, Letao Qi, Swarat Chaud...     2.0     4.0   \n",
       "918         [Nadav Cohen, Ronen Tamari, Amnon Shashua]     4.0     4.0   \n",
       "919  [Maruan Al-Shedivat, Trapit Bansal, Yura Burda...     4.0     4.0   \n",
       "92   [Dino S. Ratcliffe, Luca Citi, Sam Devlin, Udo...     3.0     4.0   \n",
       "920  [Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov...     4.0     5.0   \n",
       "921  [Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfan...     3.0     1.0   \n",
       "922  [Miltiadis Allamanis, Marc Brockschmidt, Mahmo...     4.0     4.0   \n",
       "923  [Takeru Miyato, Toshiki Kataoka, Masanori Koya...     4.0     3.0   \n",
       "924  [Ilya Tolstikhin, Olivier Bousquet, Sylvain Ge...     3.0     3.0   \n",
       "925  [Jiachen Yang, Xiaojing Ye, Rakshit Trivedi, H...     3.0     4.0   \n",
       "926        [Aman Sinha, Hongseok Namkoong, John Duchi]     5.0     4.0   \n",
       "927  [Rahul Kidambi, Praneeth Netrapalli, Prateek J...     4.0     3.0   \n",
       "928  [Christian Buck, Jannis Bulian, Massimiliano C...     5.0     3.0   \n",
       "929  [Taco S. Cohen, Mario Geiger, Jonas Köhler, Ma...     4.0     3.0   \n",
       "93            [Hu Xu, Bing Liu, Lei Shu, Philip S. Yu]     4.0     4.0   \n",
       "930  [Angeliki Lazaridou, Karl Moritz Hermann, Karl...     5.0     4.0   \n",
       "931       [Shuang Wu, Guoqi Li, Feng Chen, Luping Shi]     4.0     3.0   \n",
       "932  [Gao Huang, Danlu Chen, Tianhong Li, Felix Wu,...     4.0     4.0   \n",
       "933                   [Yonatan Belinkov, Yonatan Bisk]     4.0     4.0   \n",
       "934      [Sashank J. Reddi, Satyen Kale, Sanjiv Kumar]     5.0     4.0   \n",
       "94        [Karan Grewal, R Devon Hjelm, Yoshua Bengio]     4.0     4.0   \n",
       "95   [Matthias Bauer, Mateo Rojas-Carulla, Jakub Ba...     3.0     3.0   \n",
       "96                   [Amir Rosenfeld, John K. Tsotsos]     4.0     4.0   \n",
       "97   [David Held, Xinyang Geng, Carlos Florensa, Pi...     4.0     4.0   \n",
       "98   [Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Y...     5.0     5.0   \n",
       "99     [Kunkun Pang, Mingzhi Dong, Timothy Hospedales]     4.0     3.0   \n",
       "\n",
       "     conf_3       decision                                           keywords  \\\n",
       "0       NaN         Reject  [Causal structure discovery, Generative neural...   \n",
       "1       4.0         Reject     [hypergraph, representation learning, tensors]   \n",
       "10      4.0         Reject  [adversarial machine learning, black-box attacks]   \n",
       "100     4.0         Reject  [Distributional shift, causal effects, domain ...   \n",
       "101     2.0         Reject   [generalization, maximum entropy, deep learning]   \n",
       "102     4.0         Reject  [variational autoencoder, information theory, ...   \n",
       "103     5.0         Reject  [variational inference, vae, variational autoe...   \n",
       "104     4.0         Reject                                                 []   \n",
       "105     4.0         Reject            [deep generative models, deep learning]   \n",
       "106     3.0         Reject                                                 []   \n",
       "107     4.0         Reject                                                 []   \n",
       "108     4.0         Reject  [LSH, softmax, deep, learning, sub, linear, ef...   \n",
       "109     3.0         Reject                                                 []   \n",
       "11      4.0         Reject  [word2vec, glove, word analogy, word relations...   \n",
       "110     2.0         Reject  [topic model, Bayesian nonparametric, variatio...   \n",
       "111     4.0         Reject                                                 []   \n",
       "112     5.0         Reject  [Deep Convolutional Neural Network, Disguised ...   \n",
       "113     4.0         Reject  [Deep Learning, Robotics, Artificial Intellige...   \n",
       "114     5.0         Reject  [Applications, Security in Machine Learning, F...   \n",
       "115     5.0         Reject  [active inference, predictive coding, motor co...   \n",
       "116     5.0         Reject   [face perception, generative models, psychology]   \n",
       "117     4.0         Reject  [Object detection, Visual Tracking, Loss funct...   \n",
       "118     4.0         Reject  [deep learning, homomorphic encryption, hybrid...   \n",
       "119     2.0         Reject  [Adversarial examples, Neural Networks, Clipping]   \n",
       "12      5.0         Reject  [Knowledge Discovery, Generative Modeling, Med...   \n",
       "120     4.0         Reject  [spatial memory, egocentric vision, deep neura...   \n",
       "121     4.0         Reject                                                 []   \n",
       "122     4.0         Reject  [reinforcement learning, imitation learning, r...   \n",
       "123     2.0         Reject            [program induction, HCI, deep learning]   \n",
       "124     3.0         Reject  [Privacy-preserving deep learning, Neural netw...   \n",
       "..      ...            ...                                                ...   \n",
       "913     3.0  Accept (Oral)  [interpretability, LSTM, natural language proc...   \n",
       "914     5.0  Accept (Oral)  [imitation, zero shot, self-supervised, roboti...   \n",
       "915     4.0  Accept (Oral)  [reinforcement learning, policy gradient, vari...   \n",
       "916     4.0  Accept (Oral)  [generative adversarial networks, unsupervised...   \n",
       "917     3.0  Accept (Oral)  [Program generation, Source code, Program synt...   \n",
       "918     3.0  Accept (Oral)  [Deep Learning, Expressive Efficiency, Dilated...   \n",
       "919     2.0  Accept (Oral)  [reinforcement learning, nonstationarity, meta...   \n",
       "92      5.0         Reject  [Deep Reinforcement Learning, Domain Adaptatio...   \n",
       "920     4.0  Accept (Oral)                                                 []   \n",
       "921     4.0  Accept (Oral)  [Adversarial Subspace, Local Intrinsic Dimensi...   \n",
       "922     4.0  Accept (Oral)     [programs, source code, graph neural networks]   \n",
       "923     2.0  Accept (Oral)  [Generative Adversarial Networks, Deep Generat...   \n",
       "924     4.0  Accept (Oral)  [auto-encoder, generative models, GAN, VAE, un...   \n",
       "925     5.0  Accept (Oral)  [mean field games, reinforcement learning, Mar...   \n",
       "926     4.0  Accept (Oral)  [adversarial training, distributionally robust...   \n",
       "927     5.0  Accept (Oral)  [Stochastic Gradient Descent, Deep Learning, M...   \n",
       "928     4.0  Accept (Oral)  [machine translation, paraphrasing, question a...   \n",
       "929     4.0  Accept (Oral)  [deep learning, equivariance, convolution, gro...   \n",
       "93      4.0         Reject  [Lifelong learning, meta learning, word embedd...   \n",
       "930     4.0  Accept (Oral)  [disentanglement, communication, emergent lang...   \n",
       "931     4.0  Accept (Oral)  [quantization, training, bitwidth, ternary wei...   \n",
       "932     4.0  Accept (Oral)  [efficient learning, budgeted learning, deep l...   \n",
       "933     4.0  Accept (Oral)  [neural machine translation, characters, noise...   \n",
       "934     3.0  Accept (Oral)       [optimization, deep learning, adam, rmsprop]   \n",
       "94      3.0         Reject  [Generative Adversarial Network, Integral Prob...   \n",
       "95      3.0         Reject  [discriminative k-shot learning, probabilistic...   \n",
       "96      4.0         Reject  [Transfer Learning, Learning without forgettin...   \n",
       "97      4.0         Reject  [Reinforcement Learning, Multi-task Learning, ...   \n",
       "98      5.0         Reject  [domain adaptation, unsupervised learning, cla...   \n",
       "99      4.0         Reject     [Active Learning, Deep Reinforcement Learning]   \n",
       "\n",
       "                                             paperhash  \\\n",
       "0             goudet|causal_generative_neural_networks   \n",
       "1    sharma|hyperedge2vec_distributed_representatio...   \n",
       "10   bhagoji|exploring_the_space_of_blackbox_attack...   \n",
       "100  johansson|learning_weighted_representations_fo...   \n",
       "101  zheng|understanding_deep_learning_generalizati...   \n",
       "102  liu|preliminary_theoretical_troubleshooting_in...   \n",
       "103  kalatzis|towards_unsupervised_classification_w...   \n",
       "104  phuong|the_mutual_autoencoder_controlling_info...   \n",
       "105  hu|learning_deep_generative_models_with_discre...   \n",
       "106  che|combining_modelbased_and_modelfree_rl_via_...   \n",
       "107                    durkan|the_contextaware_learner   \n",
       "108  levy|lsh_softmax_sublinear_learning_and_infere...   \n",
       "109  blot|shade_shannon_decay_informationbased_regu...   \n",
       "11     kumar|a_closer_look_at_the_word_analogy_problem   \n",
       "110  ning|a_bayesian_nonparametric_topic_model_with...   \n",
       "111  lucas|auxiliary_guided_autoregressive_variatio...   \n",
       "112  kothawade|application_of_deep_convolutional_ne...   \n",
       "113  aly|towards_robot_vision_module_development_wi...   \n",
       "114  xu|dont_encrypt_the_data_just_approximate_the_...   \n",
       "115  daucé|toward_predictive_machine_learning_for_a...   \n",
       "116  peterson|learning_a_face_space_for_experiments...   \n",
       "117  yan|tracking_loss_converting_object_detector_t...   \n",
       "118  meehan|deep_learning_inferences_with_hybrid_ho...   \n",
       "119  addad|clipping_free_attacks_against_neural_net...   \n",
       "12   zhang|generative_discovery_of_relational_medic...   \n",
       "120            zhang|egocentric_spatial_memory_network   \n",
       "121  reddy|lsdnet_look_step_and_detect_for_joint_na...   \n",
       "122  zhu|reinforcement_and_imitation_learning_for_d...   \n",
       "123  ellis|learning_to_infer_graphics_programs_from...   \n",
       "124  li|privynet_a_flexible_framework_for_privacypr...   \n",
       "..                                                 ...   \n",
       "913  murdoch|beyond_word_importance_contextual_deco...   \n",
       "914                   pathak|zeroshot_visual_imitation   \n",
       "915  wu|variance_reduction_for_policy_gradient_with...   \n",
       "916  karras|progressive_growing_of_gans_for_improve...   \n",
       "917  murali|neural_sketch_learning_for_conditional_...   \n",
       "918  cohen|boosting_dilated_convolutional_networks_...   \n",
       "919  alshedivat|continuous_adaptation_via_metalearn...   \n",
       "92   ratcliffe|domain_adaptation_for_deep_reinforce...   \n",
       "920  yang|breaking_the_softmax_bottleneck_a_highran...   \n",
       "921  ma|characterizing_adversarial_subspaces_using_...   \n",
       "922  allamanis|learning_to_represent_programs_with_...   \n",
       "923  miyato|spectral_normalization_for_generative_a...   \n",
       "924                tolstikhin|wasserstein_autoencoders   \n",
       "925  yang|deep_mean_field_games_for_learning_optima...   \n",
       "926  sinha|certifiable_distributional_robustness_wi...   \n",
       "927  kidambi|on_the_insufficiency_of_existing_momen...   \n",
       "928  buck|ask_the_right_questions_active_question_r...   \n",
       "929                               cohen|spherical_cnns   \n",
       "93         xu|lifelong_word_embedding_via_metalearning   \n",
       "930  lazaridou|emergence_of_linguistic_communicatio...   \n",
       "931  wu|training_and_inference_with_integers_in_dee...   \n",
       "932  huang|multiscale_dense_networks_for_resource_e...   \n",
       "933  belinkov|synthetic_and_natural_noise_both_brea...   \n",
       "934        reddi|on_the_convergence_of_adam_and_beyond   \n",
       "94   grewal|variance_regularizing_adversarial_learning   \n",
       "95   bauer|discriminative_kshot_learning_using_prob...   \n",
       "96   rosenfeld|incremental_learning_through_deep_ad...   \n",
       "97   held|automatic_goal_generation_for_reinforceme...   \n",
       "98   hoffman|cycada_cycleconsistent_adversarial_dom...   \n",
       "99   pang|metalearning_transferable_active_learning...   \n",
       "\n",
       "                                                   pdf    review  review_1  \\\n",
       "0    /pdf/653017947de7c5ba987bd7db5429b9239fbac2b1.pdf       NaN       NaN   \n",
       "1    /pdf/df0bce76f679daed6584e6c2fa64e70dfeadcbb2.pdf  5.000000       5.0   \n",
       "10   /pdf/5f3238f70f31480b9668afbd0ece2e7adfe28480.pdf  6.000000       5.0   \n",
       "100  /pdf/4f50fa235bd67edc0f4fe1575d4ca3aea418f368.pdf  6.666667       5.0   \n",
       "101  /pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf  3.666667       2.0   \n",
       "102  /pdf/c8ae63388eaa3fb6206aeb98a94bc8a3d0b921e9.pdf  3.333333       5.0   \n",
       "103  /pdf/ff5681a1a9316f4ac3e53e4a01ccd192d4d6e187.pdf  4.000000       4.0   \n",
       "104  /pdf/0de2556a527a6a914b6205ba3128aef4a860889c.pdf  4.333333       4.0   \n",
       "105  /pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf  4.333333       4.0   \n",
       "106  /pdf/c94761f85f8bdbd8b9c53261e25b4ec0258406e8.pdf  4.666667       5.0   \n",
       "107  /pdf/012015f2dd7c7c2ddda23a8015bfb38251b06480.pdf  4.666667       6.0   \n",
       "108  /pdf/75e4b4dff6a768550770d7c5879285034f53de0b.pdf  5.000000       5.0   \n",
       "109  /pdf/4bef15180836618ae528217a23241f884cb4a5c7.pdf  5.333333       5.0   \n",
       "11   /pdf/f89cff1f0a3406cc6753a9c3f2ab68f629fbbef3.pdf  2.666667       2.0   \n",
       "110  /pdf/1426c0861384d97ede770bc4277cbfb13be88535.pdf  5.000000       7.0   \n",
       "111  /pdf/593becc731d948de5bea1806259dbdd34790042f.pdf  5.666667       5.0   \n",
       "112  /pdf/9b88d7b69a2d508e386ef5ef42637cf56e79b387.pdf  2.000000       1.0   \n",
       "113  /pdf/00e5c4aefc80d0396ee745c032d27e0bccb43079.pdf  2.333333       3.0   \n",
       "114  /pdf/69170f53ffe9f431f2c54cd1a453add292d356cb.pdf  3.000000       2.0   \n",
       "115  /pdf/c860906df0b1e08d5e8c36481186f1820a29f58f.pdf  3.666667       3.0   \n",
       "116  /pdf/8338680582860322884e2d86319674906ab06dbd.pdf  4.000000       3.0   \n",
       "117  /pdf/06ea09534e938f0792175ba1cb770b994107116e.pdf  4.000000       5.0   \n",
       "118  /pdf/43dae54cad481416104f335c5fb25cd47ca4a89d.pdf  4.000000       4.0   \n",
       "119  /pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf  4.000000       3.0   \n",
       "12   /pdf/1b18496cb3dd605b82c84d2db7d3ac4387f7c56d.pdf  3.333333       4.0   \n",
       "120  /pdf/56fc3f6ef511eeca73d0c621cdc76a1ea80c40fe.pdf  4.000000       3.0   \n",
       "121  /pdf/723b94906b2a6cdd548a737caefe7362143054c9.pdf  4.333333       4.0   \n",
       "122  /pdf/c39982c4ecfdad60fa6d9794ddff7574bb1cb21d.pdf  4.666667       6.0   \n",
       "123  /pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf  4.666667       4.0   \n",
       "124  /pdf/49b8492b43883423a3e61b150d43dba83d968eea.pdf  4.666667       6.0   \n",
       "..                                                 ...       ...       ...   \n",
       "913  /pdf/7ca2aa226aedab1064d25dab76f0f65e749a7b5f.pdf  7.000000       7.0   \n",
       "914  /pdf/033006fc0917363d809a60477a753aecc800ddf0.pdf  7.666667       8.0   \n",
       "915  /pdf/5c6eec975e91b7749a04d8daf58a692a38054869.pdf  7.000000       7.0   \n",
       "916  /pdf/adb9f52be84187f86cf03bf5d69e13f27c9d2d2b.pdf  5.666667       8.0   \n",
       "917  /pdf/1ca8eeb12cfbc94b3a97d0a17d39cb5a896a4294.pdf  7.333333       7.0   \n",
       "918  /pdf/71a73fa97f82c27d27f85d0ab4b65795c329ca33.pdf  8.000000       7.0   \n",
       "919  /pdf/14ddbf21a83f37283d0bc521de3d6bffaeca72dd.pdf  8.000000       8.0   \n",
       "92   /pdf/7e363a7fca433fde62ac5acc4d8fc8a55eff2a66.pdf  3.000000       3.0   \n",
       "920  /pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf  7.333333       7.0   \n",
       "921  /pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf  7.000000       8.0   \n",
       "922  /pdf/87e7ee6e015ecdafc351607a7b52e8dc87b3bc7c.pdf  8.000000       8.0   \n",
       "923  /pdf/c108c5657f8feb5338ed5bbb0ec6b430b0951880.pdf  7.333333       7.0   \n",
       "924  /pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf  8.000000       8.0   \n",
       "925  /pdf/8f628f760019758bad84d25565a929d7cc5f3d3a.pdf  5.666667       8.0   \n",
       "926  /pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf  9.000000       9.0   \n",
       "927  /pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf  7.333333       7.0   \n",
       "928  /pdf/7dfeb1f2b789a55669bbf0487c07c7bde1b47ce1.pdf  7.000000       7.0   \n",
       "929  /pdf/e8528121f5dc4d01298ed84ad29be7b64483d9b7.pdf  8.000000       8.0   \n",
       "93   /pdf/2d129dcd1101fe5b2fbb27768913ebad82c5727e.pdf  4.000000       5.0   \n",
       "930  /pdf/03978df8e51cdd7d42821961e9cd5580a4596408.pdf  7.000000       9.0   \n",
       "931  /pdf/07987f73b6973548325e92cd622d18372790e45d.pdf  7.333333       7.0   \n",
       "932  /pdf/4794d433da3a530b573404aa3995beef700360d0.pdf  5.333333       8.0   \n",
       "933  /pdf/119672f6a6a0b3fe32fdf19ddee2a270ddebfd20.pdf  7.333333       7.0   \n",
       "934  /pdf/f5e90bccde3ca55853c7482ec4c77b41aacc10e7.pdf  8.333333       9.0   \n",
       "94   /pdf/40314ec625815016cf5f00ff379932bcc1815bb8.pdf  5.000000       5.0   \n",
       "95   /pdf/c98e564bd99e7a5e1ca2dec57f97dfd703ee986f.pdf  5.000000       5.0   \n",
       "96   /pdf/85a6012f830b872e7a9e70c88a18f36383755bcd.pdf  5.000000       6.0   \n",
       "97   /pdf/d3bea8d42d0595a7bc8e7dc4edf32c95e8f6a035.pdf  6.000000       8.0   \n",
       "98   /pdf/45af0aaa43fe19af0d9e0eafa60d9252bbe3a60a.pdf  6.333333       5.0   \n",
       "99   /pdf/667b2dc6585b9e6f09bfc409b5558469556484ba.pdf  6.333333       7.0   \n",
       "\n",
       "     review_2  review_3                                              title  \\\n",
       "0         NaN       NaN                  Causal Generative Neural Networks   \n",
       "1         5.0       5.0  Hyperedge2vec: Distributed Representations for...   \n",
       "10        6.0       7.0  Exploring the Space of Black-box Attacks on De...   \n",
       "100       8.0       7.0  Learning Weighted Representations for Generali...   \n",
       "101       3.0       6.0  Understanding Deep Learning Generalization by ...   \n",
       "102       3.0       2.0  Preliminary theoretical troubleshooting in Var...   \n",
       "103       4.0       4.0  Towards Unsupervised Classification with Deep ...   \n",
       "104       5.0       4.0  The Mutual Autoencoder: Controlling Informatio...   \n",
       "105       5.0       4.0  Learning Deep Generative Models With Discrete ...   \n",
       "106       5.0       4.0  Combining Model-based and Model-free RL via Mu...   \n",
       "107       4.0       4.0                          The Context-Aware Learner   \n",
       "108       5.0       5.0  LSH Softmax: Sub-Linear Learning and Inference...   \n",
       "109       7.0       4.0  SHADE: SHAnnon DEcay Information-Based Regular...   \n",
       "11        3.0       3.0          A closer look at the word analogy problem   \n",
       "110       3.0       5.0  A Bayesian Nonparametric Topic Model with Vari...   \n",
       "111       7.0       5.0  Auxiliary Guided Autoregressive Variational Au...   \n",
       "112       2.0       3.0  APPLICATION OF DEEP CONVOLUTIONAL NEURAL NETWO...   \n",
       "113       2.0       2.0  TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH E...   \n",
       "114       4.0       3.0  Don't encrypt the data; just approximate the m...   \n",
       "115       5.0       3.0  Toward predictive machine learning for active ...   \n",
       "116       5.0       4.0  Learning a face space for experiments on human...   \n",
       "117       3.0       4.0  Tracking Loss: Converting Object Detector to R...   \n",
       "118       4.0       4.0  Deep Learning Inferences with Hybrid Homomorph...   \n",
       "119       4.0       5.0      Clipping Free Attacks Against Neural Networks   \n",
       "12        4.0       2.0  Generative Discovery of Relational Medical Ent...   \n",
       "120       5.0       4.0                  Egocentric Spatial Memory Network   \n",
       "121       6.0       3.0  LSD-Net: Look, Step and Detect for Joint Navig...   \n",
       "122       4.0       4.0  Reinforcement and Imitation Learning for Diver...   \n",
       "123       6.0       4.0  Learning to Infer Graphics Programs from Hand-...   \n",
       "124       5.0       3.0  PrivyNet: A Flexible Framework for Privacy-Pre...   \n",
       "..        ...       ...                                                ...   \n",
       "913       7.0       7.0  Beyond Word Importance:  Contextual Decomposit...   \n",
       "914       8.0       7.0                         Zero-Shot Visual Imitation   \n",
       "915       8.0       6.0  Variance Reduction for Policy Gradient with Ac...   \n",
       "916       1.0       8.0  Progressive Growing of GANs for Improved Quali...   \n",
       "917       8.0       7.0  Neural Sketch Learning for Conditional Program...   \n",
       "918       9.0       8.0  Boosting Dilated Convolutional Networks with M...   \n",
       "919       7.0       9.0  Continuous Adaptation via Meta-Learning in Non...   \n",
       "92        2.0       4.0  Domain Adaptation for Deep Reinforcement Learn...   \n",
       "920       7.0       8.0  Breaking the Softmax Bottleneck: A High-Rank R...   \n",
       "921       6.0       7.0  Characterizing Adversarial Subspaces Using Loc...   \n",
       "922       8.0       8.0         Learning to Represent Programs with Graphs   \n",
       "923       8.0       7.0  Spectral Normalization for Generative Adversar...   \n",
       "924       8.0       8.0                          Wasserstein Auto-Encoders   \n",
       "925       8.0       1.0  Deep Mean Field Games for Learning Optimal Beh...   \n",
       "926       9.0       9.0  Certifiable Distributional Robustness with Pri...   \n",
       "927       7.0       8.0  On the insufficiency of existing momentum sche...   \n",
       "928       8.0       6.0  Ask the Right Questions: Active Question Refor...   \n",
       "929       7.0       9.0                                     Spherical CNNs   \n",
       "93        3.0       4.0          Lifelong Word Embedding via Meta-Learning   \n",
       "930       7.0       5.0  Emergence of Linguistic Communication from  Re...   \n",
       "931       7.0       8.0  Training and Inference with Integers in Deep N...   \n",
       "932       7.0       1.0  Multi-Scale Dense Networks for Resource Effici...   \n",
       "933       7.0       8.0  Synthetic and Natural Noise Both Break Neural ...   \n",
       "934       8.0       8.0              On the Convergence of Adam and Beyond   \n",
       "94        4.0       6.0         Variance Regularizing Adversarial Learning   \n",
       "95        5.0       5.0  Discriminative k-shot learning using probabili...   \n",
       "96        4.0       5.0       Incremental Learning through Deep Adaptation   \n",
       "97        4.0       6.0  Automatic Goal Generation for Reinforcement Le...   \n",
       "98        5.0       9.0  CyCADA: Cycle-Consistent Adversarial Domain Ad...   \n",
       "99        6.0       6.0  Meta-Learning Transferable Active Learning Pol...   \n",
       "\n",
       "    withdrawal  \n",
       "0         None  \n",
       "1         None  \n",
       "10        None  \n",
       "100       None  \n",
       "101       None  \n",
       "102       None  \n",
       "103       None  \n",
       "104       None  \n",
       "105       None  \n",
       "106       None  \n",
       "107       None  \n",
       "108       None  \n",
       "109       None  \n",
       "11        None  \n",
       "110       None  \n",
       "111       None  \n",
       "112       None  \n",
       "113       None  \n",
       "114       None  \n",
       "115       None  \n",
       "116       None  \n",
       "117       None  \n",
       "118       None  \n",
       "119       None  \n",
       "12        None  \n",
       "120       None  \n",
       "121       None  \n",
       "122       None  \n",
       "123       None  \n",
       "124       None  \n",
       "..         ...  \n",
       "913       None  \n",
       "914       None  \n",
       "915       None  \n",
       "916       None  \n",
       "917       None  \n",
       "918       None  \n",
       "919       None  \n",
       "92        None  \n",
       "920       None  \n",
       "921       None  \n",
       "922       None  \n",
       "923       None  \n",
       "924       None  \n",
       "925       None  \n",
       "926       None  \n",
       "927       None  \n",
       "928       None  \n",
       "929       None  \n",
       "93        None  \n",
       "930       None  \n",
       "931       None  \n",
       "932       None  \n",
       "933       None  \n",
       "934       None  \n",
       "94        None  \n",
       "95        None  \n",
       "96        None  \n",
       "97        None  \n",
       "98        None  \n",
       "99        None  \n",
       "\n",
       "[935 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"./save_iclr.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "[('fr', 4), ('com', 1), ('org', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('sg', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('cn', 3)]\n",
      "========================================\n",
      "[('cn', 2), ('com', 2)]\n",
      "========================================\n",
      "[('gr', 4), ('com', 2), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 3), ('at', 1), ('nl', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1), ('ca', 1)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('fr', 4)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('com', 4), ('cn', 2)]\n",
      "========================================\n",
      "[('fr', 2)]\n",
      "========================================\n",
      "[('in', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 1)]\n",
      "========================================\n",
      "[('fr', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 2), ('hk', 2)]\n",
      "========================================\n",
      "[('nz', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 1), ('fr', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2)]\n",
      "========================================\n",
      "[('sg', 4), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 1)]\n",
      "========================================\n",
      "[('com', 10), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2)]\n",
      "========================================\n",
      "[('hk', 1)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('edu', 4), ('cn', 1)]\n",
      "========================================\n",
      "[('com', 5), ('de', 3)]\n",
      "========================================\n",
      "[('nl', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('ch', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('uk', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('sg', 3)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 2), ('tw', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('au', 4)]\n",
      "========================================\n",
      "[('kr', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('ru', 3), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2)]\n",
      "========================================\n",
      "[('com', 1), ('es', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('cn', 5)]\n",
      "========================================\n",
      "[('ca', 3)]\n",
      "========================================\n",
      "[('com', 10)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 8)]\n",
      "========================================\n",
      "[('edu', 4), ('cn', 1)]\n",
      "========================================\n",
      "[('de', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 10)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('uk', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('ch', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 3), ('de', 2)]\n",
      "========================================\n",
      "[('com', 1), ('gov', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('com', 2), ('ca', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('ai', 4)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('tw', 4), ('com', 1)]\n",
      "========================================\n",
      "[('gov', 4), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 3), ('cn', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('com', 1), ('uk', 1)]\n",
      "========================================\n",
      "[('br', 3)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('cn', 4)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('de', 1)]\n",
      "========================================\n",
      "[('ch', 2)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('fr', 3)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('ca', 2)]\n",
      "========================================\n",
      "[('com', 3), ('io', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('uk', 4), ('com', 1)]\n",
      "========================================\n",
      "[('com', 8)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('jp', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 1)]\n",
      "========================================\n",
      "[('uk', 2)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('cc/2018/Conference/Paper239/Authors', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('cn', 4), ('tw', 1), ('ca', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 1), ('kr', 1)]\n",
      "========================================\n",
      "[('cn', 6), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 1), ('de', 1)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('il', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('in', 1)]\n",
      "========================================\n",
      "[('com', 3), ('ca', 1)]\n",
      "========================================\n",
      "[('gov', 3)]\n",
      "========================================\n",
      "[('mil', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('cn', 3)]\n",
      "========================================\n",
      "[('ca', 2), ('com', 1), ('hk', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('de', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 7)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1), ('org', 1)]\n",
      "========================================\n",
      "[('com', 2), ('de', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('in', 5)]\n",
      "========================================\n",
      "[('edu', 1), ('ca', 1)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('edu', 3), ('es', 1), ('ch', 1)]\n",
      "========================================\n",
      "[('ch', 3)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('fr', 2)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 1), ('tw', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 6)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('com', 5), ('uk', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 2)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 6), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('cn', 2), ('com', 2)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('com', 1), ('ca', 1)]\n",
      "========================================\n",
      "[('uk', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('se', 1), ('one', 1)]\n",
      "========================================\n",
      "[('com', 2), ('fr', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('cn', 1)]\n",
      "========================================\n",
      "[('edu', 1)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2)]\n",
      "========================================\n",
      "[('il', 3)]\n",
      "========================================\n",
      "[('com', 1), ('tw', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('uk', 2)]\n",
      "========================================\n",
      "[('tw', 2)]\n",
      "========================================\n",
      "[('uk', 1), ('com', 1)]\n",
      "========================================\n",
      "[('it', 2), ('de', 2), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 4), ('cn', 2)]\n",
      "========================================\n",
      "[('com', 3), ('cn', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('es', 6), ('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('kr', 3)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('uk', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2)]\n",
      "========================================\n",
      "[('cn', 2), ('edu', 1), ('uk', 1)]\n",
      "========================================\n",
      "[('uk', 2)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('kr', 4)]\n",
      "========================================\n",
      "[('au', 2)]\n",
      "========================================\n",
      "[('com', 1), ('org', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 5), ('ca', 1), ('uk', 1)]\n",
      "========================================\n",
      "[('de', 4)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('ca', 2)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('fr', 3), ('com', 1)]\n",
      "========================================\n",
      "[('de', 3)]\n",
      "========================================\n",
      "[('jp', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('ch', 2)]\n",
      "========================================\n",
      "[('edu', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('ch', 3)]\n",
      "========================================\n",
      "[('hk', 3)]\n",
      "========================================\n",
      "[('com', 7)]\n",
      "========================================\n",
      "[('cn', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('cn', 2), ('edu', 2)]\n",
      "========================================\n",
      "[('de', 3)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 4), ('cn', 1)]\n",
      "========================================\n",
      "[('cn', 2), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 1)]\n",
      "========================================\n",
      "[('com', 4), ('tw', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 1)]\n",
      "========================================\n",
      "[('fr', 3)]\n",
      "========================================\n",
      "[('cn', 7)]\n",
      "========================================\n",
      "[('uk', 2)]\n",
      "========================================\n",
      "[('ai', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('tr', 2)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 2)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('ch', 4)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1), ('org', 1)]\n",
      "========================================\n",
      "[('com', 3), ('ch', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 6), ('at', 1)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('es', 2), ('cat', 2)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('ch', 2), ('uk', 1)]\n",
      "========================================\n",
      "[('de', 3)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('com', 4), ('ca', 1)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('be', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('ca', 3)]\n",
      "========================================\n",
      "[('eg', 3)]\n",
      "========================================\n",
      "[('com', 2), ('tr', 1)]\n",
      "========================================\n",
      "[('au', 3)]\n",
      "========================================\n",
      "[('com', 3), ('org', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('ch', 5)]\n",
      "========================================\n",
      "[('com', 2), ('cn', 2)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('tr', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('sg', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 5), ('sa', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('uk', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('kr', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('uk', 1), ('com', 1), ('org', 1)]\n",
      "========================================\n",
      "[('com', 2), ('org', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('com', 2), ('de', 1)]\n",
      "========================================\n",
      "[('uk', 1), ('org', 1)]\n",
      "========================================\n",
      "[('edu', 7), ('com', 1)]\n",
      "========================================\n",
      "[('br', 3)]\n",
      "========================================\n",
      "[('kr', 5)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 2), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 5), ('ai', 1)]\n",
      "========================================\n",
      "[('br', 4)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('ch', 4), ('com', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 1), ('org', 1)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('ca', 2), ('cn', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('de', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('de', 2)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('jp', 2)]\n",
      "========================================\n",
      "[('jp', 3)]\n",
      "========================================\n",
      "[('com', 5), ('edu', 1)]\n",
      "========================================\n",
      "[('ch', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('il', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2), ('org', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 5), ('com', 1)]\n",
      "========================================\n",
      "[('de', 3)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('ch', 3)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('fr', 2), ('com', 1), ('gr', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1), ('fr', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 6)]\n",
      "========================================\n",
      "[('edu', 7)]\n",
      "========================================\n",
      "[('edu', 2), ('cn', 1)]\n",
      "========================================\n",
      "[('com', 1), ('de', 1)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('ca', 2)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('jp', 3)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('dk', 2), ('org', 1)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 4), ('cn', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('net', 1)]\n",
      "========================================\n",
      "[('kr', 3)]\n",
      "========================================\n",
      "[('com', 2), ('dk', 1)]\n",
      "========================================\n",
      "[('cn', 2)]\n",
      "========================================\n",
      "[('com', 3), ('cn', 1)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('it', 1), ('uk', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 2)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 2), ('ca', 1)]\n",
      "========================================\n",
      "[('nl', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('ca', 2)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('dk', 2)]\n",
      "========================================\n",
      "[('jp', 2)]\n",
      "========================================\n",
      "[('kr', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('tw', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('ca', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2)]\n",
      "========================================\n",
      "[('eu', 1), ('com', 1), ('fr', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('no', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('com', 2), ('ca', 1), ('fr', 1)]\n",
      "========================================\n",
      "[('sg', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('uk', 4), ('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('ch', 3)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('jp', 4), ('org', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 1), ('ru', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('tw', 3)]\n",
      "========================================\n",
      "[('ca', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('uk', 4)]\n",
      "========================================\n",
      "[('jp', 4)]\n",
      "========================================\n",
      "[('de', 2), ('ch', 1)]\n",
      "========================================\n",
      "[('com', 3), ('name', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 2), ('ca', 1)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 5), ('com', 2)]\n",
      "========================================\n",
      "[('at', 2)]\n",
      "========================================\n",
      "[('cn', 2)]\n",
      "========================================\n",
      "[('com', 3), ('ca', 1)]\n",
      "========================================\n",
      "[('com', 5), ('cn', 2)]\n",
      "========================================\n",
      "[('de', 3), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 2), ('de', 1)]\n",
      "========================================\n",
      "[('nl', 3), ('it', 1)]\n",
      "========================================\n",
      "[('ch', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 5), ('ca', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('fr', 4)]\n",
      "========================================\n",
      "[('edu', 1)]\n",
      "========================================\n",
      "[('br', 3)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('de', 2)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 5), ('cn', 1)]\n",
      "========================================\n",
      "[('jp', 3)]\n",
      "========================================\n",
      "[('edu', 5), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('uk', 4)]\n",
      "========================================\n",
      "[('com', 2), ('de', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1), ('org', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('jp', 4)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('il', 3)]\n",
      "========================================\n",
      "[('edu', 1), ('it', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 6), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('com', 4), ('il', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 3), ('in', 1)]\n",
      "========================================\n",
      "[('ca', 3), ('com', 2), ('fr', 1)]\n",
      "========================================\n",
      "[('in', 3)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('org', 7), ('edu', 4)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('ca', 1)]\n",
      "========================================\n",
      "[('ca', 3)]\n",
      "========================================\n",
      "[('com', 7), ('org', 1)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('jp', 6), ('com', 2)]\n",
      "========================================\n",
      "[('com', 4), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 4), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1), ('io', 1)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('edu', 6)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 4), ('edu', 1), ('sg', 1)]\n",
      "========================================\n",
      "[('kr', 3)]\n",
      "========================================\n",
      "[('edu', 5), ('pt', 1)]\n",
      "========================================\n",
      "[('cn', 3)]\n",
      "========================================\n",
      "[('de', 1), ('com', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('ch', 5)]\n",
      "========================================\n",
      "[('com', 4), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 2), ('cn', 2), ('ai', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('de', 2), ('com', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('ai', 7)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('cn', 2), ('ca', 2)]\n",
      "========================================\n",
      "[('ai', 2)]\n",
      "========================================\n",
      "[('in', 2), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('cn', 1)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('cn', 4)]\n",
      "========================================\n",
      "[('com', 4), ('ca', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 3)]\n",
      "========================================\n",
      "[('ch', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 6)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 9), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('cn', 1)]\n",
      "========================================\n",
      "[('cn', 2), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 2), ('org', 1)]\n",
      "========================================\n",
      "[('sg', 3)]\n",
      "========================================\n",
      "[('com', 1), ('cn', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('cn', 4), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 2), ('ca', 1)]\n",
      "========================================\n",
      "[('kr', 2), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 6), ('uk', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('net', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 17)]\n",
      "========================================\n",
      "[('uk', 5)]\n",
      "========================================\n",
      "[('uk', 4)]\n",
      "========================================\n",
      "[('nl', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1), ('de', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('de', 2), ('com', 2)]\n",
      "========================================\n",
      "[('com', 2), ('nl', 2)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1), ('uk', 1)]\n",
      "========================================\n",
      "[('in', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 8)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1), ('org', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 2), ('ca', 2)]\n",
      "========================================\n",
      "[('edu', 6)]\n",
      "========================================\n",
      "[('de', 2)]\n",
      "========================================\n",
      "[('jp', 3)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 4), ('ca', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('de', 2), ('com', 1)]\n",
      "========================================\n",
      "[('ch', 2), ('edu', 2), ('fr', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('uk', 4), ('ca', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('jp', 2), ('pt', 1)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('net', 1), ('com', 1)]\n",
      "========================================\n",
      "[('cn', 3), ('com', 3)]\n",
      "========================================\n",
      "[('com', 3), ('uk', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('uk', 1)]\n",
      "========================================\n",
      "[('il', 3)]\n",
      "========================================\n",
      "[('de', 2), ('com', 2)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 7), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 3), ('hk', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('uk', 4), ('com', 1)]\n",
      "========================================\n",
      "[('it', 2), ('edu', 1), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 7), ('edu', 2)]\n",
      "========================================\n",
      "[('uk', 3), ('com', 1)]\n",
      "========================================\n",
      "[('nl', 4), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 2), ('ru', 1)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('il', 4)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('in', 4)]\n",
      "========================================\n",
      "[('uk', 2)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 2), ('cn', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('ch', 4)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('ca', 3), ('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 4), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('com', 8), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 3)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('de', 3), ('in', 1)]\n",
      "========================================\n",
      "[('de', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('de', 4), ('com', 3)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('hk', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 4), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 4), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('kr', 4)]\n",
      "========================================\n",
      "[('org', 4), ('edu', 3)]\n",
      "========================================\n",
      "[('edu', 7), ('org', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 6), ('edu', 3)]\n",
      "========================================\n",
      "[('com', 3), ('ca', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('at', 7)]\n",
      "========================================\n",
      "[('cn', 6), ('com', 1), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('fr', 3)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1), ('org', 1)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('com', 3), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 3)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('cc', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('ca', 2), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 4), ('fr', 1)]\n",
      "========================================\n",
      "[('com', 2), ('org', 1)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1), ('org', 1)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('il', 4)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('de', 4)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('uk', 4)]\n",
      "========================================\n",
      "[('au', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('de', 4)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('au', 4), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('ca', 8), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('in', 4), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 6), ('com', 2)]\n",
      "========================================\n",
      "[('uk', 2)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 2), ('ch', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 10)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 8)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('ch', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('kr', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 5), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2)]\n",
      "========================================\n",
      "[('il', 2)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('fr', 2)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 3), ('ca', 1)]\n",
      "========================================\n",
      "[('at', 4)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 7)]\n",
      "========================================\n",
      "[('com', 5), ('edu', 1)]\n",
      "========================================\n",
      "[('nl', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 13)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('ca', 3)]\n",
      "========================================\n",
      "[('in', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('ca', 2), ('com', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 4), ('me', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2), ('ca', 1)]\n",
      "========================================\n",
      "[('uk', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 10)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1), ('org', 1)]\n",
      "========================================\n",
      "[('ca', 3), ('com', 1), ('ai', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('kr', 1)]\n",
      "========================================\n",
      "[('ai', 4), ('com', 2)]\n",
      "========================================\n",
      "[('edu', 6)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 2)]\n",
      "========================================\n",
      "[('fr', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 4)]\n",
      "========================================\n",
      "[('com', 1), ('nl', 1), ('fr', 1)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('com', 2), ('ch', 1)]\n",
      "========================================\n",
      "[('com', 5), ('uk', 1)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('cn', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 4), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('se', 3)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('es', 2), ('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('ch', 4)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('ch', 6)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 1)]\n",
      "========================================\n",
      "[('de', 1), ('org', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('uk', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 3)]\n",
      "========================================\n",
      "[('kr', 2), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 1), ('org', 1)]\n",
      "========================================\n",
      "[('org', 3)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 1), ('sg', 1), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('es', 2), ('io', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('eus', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 5), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 4), ('cn', 1)]\n",
      "========================================\n",
      "[('jp', 2)]\n",
      "========================================\n",
      "[('com', 2), ('org', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 3), ('fr', 1)]\n",
      "========================================\n",
      "[('com', 7)]\n",
      "========================================\n",
      "[('com', 6), ('edu', 1), ('ca', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('kr', 2), ('edu', 2)]\n",
      "========================================\n",
      "[('jp', 3)]\n",
      "========================================\n",
      "[('com', 7)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('com', 3), ('ca', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('uk', 2), ('com', 1)]\n",
      "========================================\n",
      "[('cn', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 3)]\n",
      "========================================\n",
      "[('jp', 3)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('edu', 3), ('pt', 1)]\n",
      "========================================\n",
      "[('fr', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('uk', 3)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('jp', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('pl', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('fr', 3)]\n",
      "========================================\n",
      "[('jp', 1), ('com', 1)]\n",
      "========================================\n",
      "[('kr', 3), ('com', 2)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 3)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 9)]\n",
      "========================================\n",
      "[('fr', 2), ('ru', 1), ('ca', 1)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('au', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('eu', 3), ('com', 2), ('cn', 2)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 8), ('sg', 2)]\n",
      "========================================\n",
      "[('com', 4), ('edu', 2)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 1)]\n",
      "========================================\n",
      "[('com', 6)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 2)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('in', 4)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 11)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 2)]\n",
      "========================================\n",
      "[('ch', 5)]\n",
      "========================================\n",
      "[('com', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 1), ('edu', 1)]\n",
      "========================================\n",
      "[('com', 4), ('uk', 2)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 1)]\n",
      "========================================\n",
      "[('uk', 4), ('ai', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 2), ('ca', 2)]\n",
      "========================================\n",
      "[('edu', 3), ('cn', 2), ('com', 1)]\n",
      "========================================\n",
      "[('ca', 3)]\n",
      "========================================\n",
      "[('edu', 2)]\n",
      "========================================\n",
      "[('com', 4), ('uk', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('com', 6), ('edu', 1)]\n",
      "========================================\n",
      "[('dk', 3)]\n",
      "========================================\n",
      "[('nl', 2), ('de', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('com', 4), ('edu', 1)]\n",
      "========================================\n",
      "[('ch', 3)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('fr', 3)]\n",
      "========================================\n",
      "[('fr', 4)]\n",
      "========================================\n",
      "[('com', 5)]\n",
      "========================================\n",
      "[('com', 3), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1), ('nl', 1)]\n",
      "========================================\n",
      "[('com', 2)]\n",
      "========================================\n",
      "[('edu', 5)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('fr', 3), ('com', 2), ('jp', 1)]\n",
      "========================================\n",
      "[('it', 3)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 10)]\n",
      "========================================\n",
      "[('edu', 7), ('com', 1)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('il', 2), ('edu', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 3)]\n",
      "========================================\n",
      "[('uk', 4)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 1)]\n",
      "========================================\n",
      "[('au', 4), ('edu', 3), ('cn', 1)]\n",
      "========================================\n",
      "[('com', 2), ('ca', 1)]\n",
      "========================================\n",
      "[('jp', 3), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3), ('de', 1)]\n",
      "========================================\n",
      "[('edu', 4), ('com', 1)]\n",
      "========================================\n",
      "[('edu', 3)]\n",
      "========================================\n",
      "[('edu', 2), ('com', 2)]\n",
      "========================================\n",
      "[('com', 7)]\n",
      "========================================\n",
      "[('com', 3), ('nl', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('com', 4)]\n",
      "========================================\n",
      "[('cn', 1)]\n",
      "========================================\n",
      "[('edu', 3), ('com', 2), ('cn', 1)]\n",
      "========================================\n",
      "[('edu', 1), ('com', 1)]\n",
      "========================================\n",
      "[('com', 3)]\n",
      "========================================\n",
      "[('com', 2), ('ca', 1)]\n",
      "========================================\n",
      "[('uk', 2), ('com', 2), ('de', 1)]\n",
      "========================================\n",
      "[('com', 1)]\n",
      "========================================\n",
      "[('edu', 4)]\n",
      "========================================\n",
      "[('edu', 8)]\n",
      "========================================\n",
      "[('uk', 3)]\n"
     ]
    }
   ],
   "source": [
    "list_inst = []\n",
    "for list_id in df.authorids:\n",
    "    #print(list_id)\n",
    "    print('========================================')\n",
    "    #for id in list_id:\n",
    "    #    inst = id.split('@')[-1]\n",
    "        #print(inst)\n",
    "    inst = [id.split('.')[-1] for id in list_id]\n",
    "    list_inst = list_inst + inst\n",
    "    count_dict = Counter(inst)\n",
    "    print(count_dict.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_dic = {}\n",
    "for inst_list,review, decision in zip(df.authorids,df.review, df.decision):\n",
    "    if review!=review:\n",
    "        continue\n",
    "    inst_list = [id.split('.')[-1] for id in inst_list]\n",
    "    list_already_seen = []\n",
    "    for inst in inst_list:\n",
    "        if inst in inst_dic and inst not in list_already_seen:\n",
    "            # count\n",
    "            inst_dic[inst][1] +=1\n",
    "            # note\n",
    "            inst_dic[inst][2] +=review\n",
    "            # Reject\n",
    "            if decision == 'Reject':\n",
    "                inst_dic[inst][3] += 1\n",
    "                inst_dic[inst][4] += review\n",
    "                \n",
    "            # Workshop\n",
    "            if decision == 'Invite to Workshop Track':\n",
    "                inst_dic[inst][5] += 1\n",
    "                inst_dic[inst][6] += review\n",
    "            # Poster\n",
    "            if decision == 'Accept (Poster)':\n",
    "                inst_dic[inst][7] += 1\n",
    "                inst_dic[inst][8] += review\n",
    "            # Oral\n",
    "            if decision == 'Accept (Oral)':\n",
    "                inst_dic[inst][9] += 1\n",
    "                inst_dic[inst][10] += review\n",
    "        else:\n",
    "            if inst not in list_already_seen:\n",
    "                \n",
    "                inst_dic[inst] = [0, 0,0,0,0,0,0,0,0,0,0]\n",
    "                # count\n",
    "                inst_dic[inst][1] +=1\n",
    "                # note\n",
    "                inst_dic[inst][2] +=review\n",
    "                # count\n",
    "                # Reject\n",
    "                if decision == 'Reject':\n",
    "                    inst_dic[inst][3] += 1\n",
    "                    inst_dic[inst][4] += review\n",
    "\n",
    "                # Workshop\n",
    "                if decision == 'Invite to Workshop Track':\n",
    "                    inst_dic[inst][5] += 1\n",
    "                    inst_dic[inst][6] += review\n",
    "                # Poster\n",
    "                if decision == 'Accept (Poster)':\n",
    "                    inst_dic[inst][7] += 1\n",
    "                    inst_dic[inst][8] += review\n",
    "                # Oral\n",
    "                if decision == 'Accept (Oral)':\n",
    "                    inst_dic[inst][9] += 1\n",
    "                    inst_dic[inst][10] += review\n",
    "        inst_dic[inst][0] += 1\n",
    "        list_already_seen.append(inst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, values in inst_dic.items():\n",
    "    if values[1] > 0:\n",
    "        values[0] /= values[1]\n",
    "    if values[2] > 0:\n",
    "        values[2] /= values[1]\n",
    "    if values[4] > 0:\n",
    "        values[4] /= values[3]\n",
    "    if values[6] > 0:\n",
    "        values[6] /= values[5]\n",
    "    if values[8] > 0:\n",
    "        values[8] /= values[7]\n",
    "    if values[10] > 0:\n",
    "        values[10] /= values[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for key, values in inst_dic.items():\n",
    "    df_temp = pd.DataFrame({'nb_paper':values[1],'avg_note':values[2],'nb_reject':values[3],'avg_reject':values[4],'nb_workshop':values[5],'avg_workshop':values[6],'nb_poster':values[7],'avg_poster':values[8],'nb_oral':values[9],'avg_oral':values[10]} ,index=[key])\n",
    "    df_list.append(df_temp)\n",
    "df_inst = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_note</th>\n",
       "      <th>avg_oral</th>\n",
       "      <th>avg_poster</th>\n",
       "      <th>avg_reject</th>\n",
       "      <th>avg_workshop</th>\n",
       "      <th>nb_oral</th>\n",
       "      <th>nb_paper</th>\n",
       "      <th>nb_poster</th>\n",
       "      <th>nb_reject</th>\n",
       "      <th>nb_workshop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>com</th>\n",
       "      <td>5.512671</td>\n",
       "      <td>7.145833</td>\n",
       "      <td>6.485320</td>\n",
       "      <td>4.724359</td>\n",
       "      <td>5.310606</td>\n",
       "      <td>16</td>\n",
       "      <td>513</td>\n",
       "      <td>193</td>\n",
       "      <td>260</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edu</th>\n",
       "      <td>5.532885</td>\n",
       "      <td>7.238095</td>\n",
       "      <td>6.501035</td>\n",
       "      <td>4.752252</td>\n",
       "      <td>5.401361</td>\n",
       "      <td>14</td>\n",
       "      <td>446</td>\n",
       "      <td>161</td>\n",
       "      <td>222</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uk</th>\n",
       "      <td>5.328125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.280000</td>\n",
       "      <td>4.648649</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca</th>\n",
       "      <td>5.236111</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.604167</td>\n",
       "      <td>4.240000</td>\n",
       "      <td>5.277778</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>5.654762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>4.904762</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>5.368421</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>4.782609</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch</th>\n",
       "      <td>5.309524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.370370</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org</th>\n",
       "      <td>5.525641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.629630</td>\n",
       "      <td>4.888889</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cn</th>\n",
       "      <td>5.180000</td>\n",
       "      <td>6.555556</td>\n",
       "      <td>6.523810</td>\n",
       "      <td>4.644444</td>\n",
       "      <td>5.433333</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jp</th>\n",
       "      <td>5.714286</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>6.571429</td>\n",
       "      <td>4.814815</td>\n",
       "      <td>5.444444</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>5.916667</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.944444</td>\n",
       "      <td>4.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kr</th>\n",
       "      <td>5.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.388889</td>\n",
       "      <td>4.370370</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>il</th>\n",
       "      <td>6.133333</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>4.888889</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>au</th>\n",
       "      <td>5.857143</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>5.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.777778</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>5.296296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sg</th>\n",
       "      <td>5.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>4.866667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>4.888889</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>6.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hk</th>\n",
       "      <td>5.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>5.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>io</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dk</th>\n",
       "      <td>4.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net</th>\n",
       "      <td>5.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eus</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>br</th>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gr</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nz</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tw</th>\n",
       "      <td>5.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gov</th>\n",
       "      <td>3.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc/2018/Conference/Paper239/Authors</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mil</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>se</th>\n",
       "      <td>5.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eg</th>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sa</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eu</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     avg_note  avg_oral  avg_poster  \\\n",
       "com                                  5.512671  7.145833    6.485320   \n",
       "edu                                  5.532885  7.238095    6.501035   \n",
       "uk                                   5.328125  0.000000    6.280000   \n",
       "ca                                   5.236111  8.000000    6.604167   \n",
       "fr                                   5.654762  0.000000    6.750000   \n",
       "de                                   5.368421  8.000000    6.500000   \n",
       "ch                                   5.309524  0.000000    6.370370   \n",
       "org                                  5.525641  0.000000    6.629630   \n",
       "cn                                   5.180000  6.555556    6.523810   \n",
       "jp                                   5.714286  7.166667    6.571429   \n",
       "nl                                   5.916667  8.000000    6.944444   \n",
       "kr                                   5.187500  0.000000    6.388889   \n",
       "il                                   6.133333  8.000000    6.833333   \n",
       "au                                   5.857143  7.000000    6.333333   \n",
       "in                                   5.166667  0.000000    6.777778   \n",
       "ai                                   5.296296  0.000000    7.166667   \n",
       "sg                                   5.185185  0.000000    6.166667   \n",
       "it                                   5.666667  0.000000    6.500000   \n",
       "pt                                   6.222222  0.000000    6.333333   \n",
       "at                                   5.200000  0.000000    6.333333   \n",
       "hk                                   5.222222  0.000000    6.666667   \n",
       "ru                                   5.666667  0.000000    6.333333   \n",
       "es                                   5.055556  0.000000    6.166667   \n",
       "io                                   5.000000  0.000000    6.333333   \n",
       "dk                                   4.416667  0.000000    5.666667   \n",
       "net                                  5.444444  0.000000    6.666667   \n",
       "cc                                   5.666667  0.000000    5.666667   \n",
       "me                                   6.000000  0.000000    6.000000   \n",
       "eus                                  6.000000  0.000000    6.000000   \n",
       "br                                   3.666667  0.000000    0.000000   \n",
       "gr                                   4.333333  0.000000    0.000000   \n",
       "nz                                   4.000000  0.000000    0.000000   \n",
       "tw                                   5.037037  0.000000    0.000000   \n",
       "gov                                  3.777778  0.000000    0.000000   \n",
       "cc/2018/Conference/Paper239/Authors  4.666667  0.000000    0.000000   \n",
       "mil                                  4.000000  0.000000    0.000000   \n",
       "se                                   5.166667  0.000000    0.000000   \n",
       "one                                  5.000000  0.000000    0.000000   \n",
       "tr                                   4.333333  0.000000    0.000000   \n",
       "cat                                  5.333333  0.000000    0.000000   \n",
       "be                                   5.333333  0.000000    0.000000   \n",
       "eg                                   3.666667  0.000000    0.000000   \n",
       "sa                                   5.666667  0.000000    0.000000   \n",
       "eu                                   4.666667  0.000000    0.000000   \n",
       "no                                   3.000000  0.000000    0.000000   \n",
       "name                                 4.333333  0.000000    0.000000   \n",
       "pl                                   3.000000  0.000000    0.000000   \n",
       "\n",
       "                                     avg_reject  avg_workshop  nb_oral  \\\n",
       "com                                    4.724359      5.310606       16   \n",
       "edu                                    4.752252      5.401361       14   \n",
       "uk                                     4.648649      6.000000        0   \n",
       "ca                                     4.240000      5.277778        1   \n",
       "fr                                     4.904762      4.333333        0   \n",
       "de                                     4.782609      5.250000        1   \n",
       "ch                                     4.705882      5.666667        0   \n",
       "org                                    4.888889      5.333333        0   \n",
       "cn                                     4.644444      5.433333        3   \n",
       "jp                                     4.814815      5.444444        2   \n",
       "nl                                     4.266667      0.000000        1   \n",
       "kr                                     4.370370      5.333333        0   \n",
       "il                                     4.888889      5.666667        1   \n",
       "au                                     5.000000      0.000000        1   \n",
       "in                                     4.333333      5.666667        0   \n",
       "ai                                     4.083333      5.666667        0   \n",
       "sg                                     4.866667      5.000000        0   \n",
       "it                                     4.888889      6.333333        0   \n",
       "pt                                     0.000000      6.000000        0   \n",
       "at                                     4.444444      0.000000        0   \n",
       "hk                                     4.500000      0.000000        0   \n",
       "ru                                     5.000000      0.000000        0   \n",
       "es                                     4.500000      0.000000        0   \n",
       "io                                     4.000000      4.666667        0   \n",
       "dk                                     4.000000      0.000000        0   \n",
       "net                                    4.833333      0.000000        0   \n",
       "cc                                     0.000000      0.000000        0   \n",
       "me                                     0.000000      0.000000        0   \n",
       "eus                                    0.000000      0.000000        0   \n",
       "br                                     3.222222      5.000000        0   \n",
       "gr                                     4.333333      0.000000        0   \n",
       "nz                                     4.000000      0.000000        0   \n",
       "tw                                     5.037037      0.000000        0   \n",
       "gov                                    3.777778      0.000000        0   \n",
       "cc/2018/Conference/Paper239/Authors    4.666667      0.000000        0   \n",
       "mil                                    4.000000      0.000000        0   \n",
       "se                                     5.166667      0.000000        0   \n",
       "one                                    5.000000      0.000000        0   \n",
       "tr                                     4.333333      0.000000        0   \n",
       "cat                                    5.333333      0.000000        0   \n",
       "be                                     5.333333      0.000000        0   \n",
       "eg                                     3.666667      0.000000        0   \n",
       "sa                                     5.666667      0.000000        0   \n",
       "eu                                     4.666667      0.000000        0   \n",
       "no                                     3.000000      0.000000        0   \n",
       "name                                   4.333333      0.000000        0   \n",
       "pl                                     3.000000      0.000000        0   \n",
       "\n",
       "                                     nb_paper  nb_poster  nb_reject  \\\n",
       "com                                       513        193        260   \n",
       "edu                                       446        161        222   \n",
       "uk                                         64         25         37   \n",
       "ca                                         48         16         25   \n",
       "fr                                         28         12         14   \n",
       "de                                         38         10         23   \n",
       "ch                                         28          9         17   \n",
       "org                                        26          9         15   \n",
       "cn                                         50          7         30   \n",
       "jp                                         21          7          9   \n",
       "nl                                         12          6          5   \n",
       "kr                                         16          6          9   \n",
       "il                                         10          4          3   \n",
       "au                                          7          3          3   \n",
       "in                                         12          3          7   \n",
       "ai                                          9          2          4   \n",
       "sg                                          9          2          5   \n",
       "it                                          6          2          3   \n",
       "pt                                          3          2          0   \n",
       "at                                          5          2          3   \n",
       "hk                                          6          2          4   \n",
       "ru                                          4          2          2   \n",
       "es                                          6          2          4   \n",
       "io                                          3          1          1   \n",
       "dk                                          4          1          3   \n",
       "net                                         3          1          2   \n",
       "cc                                          1          1          0   \n",
       "me                                          1          1          0   \n",
       "eus                                         1          1          0   \n",
       "br                                          4          0          3   \n",
       "gr                                          2          0          2   \n",
       "nz                                          1          0          1   \n",
       "tw                                          9          0          9   \n",
       "gov                                         3          0          3   \n",
       "cc/2018/Conference/Paper239/Authors         1          0          1   \n",
       "mil                                         1          0          1   \n",
       "se                                          2          0          2   \n",
       "one                                         1          0          1   \n",
       "tr                                          3          0          3   \n",
       "cat                                         1          0          1   \n",
       "be                                          1          0          1   \n",
       "eg                                          1          0          1   \n",
       "sa                                          1          0          1   \n",
       "eu                                          2          0          2   \n",
       "no                                          1          0          1   \n",
       "name                                        1          0          1   \n",
       "pl                                          1          0          1   \n",
       "\n",
       "                                     nb_workshop  \n",
       "com                                           44  \n",
       "edu                                           49  \n",
       "uk                                             2  \n",
       "ca                                             6  \n",
       "fr                                             2  \n",
       "de                                             4  \n",
       "ch                                             2  \n",
       "org                                            2  \n",
       "cn                                            10  \n",
       "jp                                             3  \n",
       "nl                                             0  \n",
       "kr                                             1  \n",
       "il                                             2  \n",
       "au                                             0  \n",
       "in                                             2  \n",
       "ai                                             3  \n",
       "sg                                             2  \n",
       "it                                             1  \n",
       "pt                                             1  \n",
       "at                                             0  \n",
       "hk                                             0  \n",
       "ru                                             0  \n",
       "es                                             0  \n",
       "io                                             1  \n",
       "dk                                             0  \n",
       "net                                            0  \n",
       "cc                                             0  \n",
       "me                                             0  \n",
       "eus                                            0  \n",
       "br                                             1  \n",
       "gr                                             0  \n",
       "nz                                             0  \n",
       "tw                                             0  \n",
       "gov                                            0  \n",
       "cc/2018/Conference/Paper239/Authors            0  \n",
       "mil                                            0  \n",
       "se                                             0  \n",
       "one                                            0  \n",
       "tr                                             0  \n",
       "cat                                            0  \n",
       "be                                             0  \n",
       "eg                                             0  \n",
       "sa                                             0  \n",
       "eu                                             0  \n",
       "no                                             0  \n",
       "name                                           0  \n",
       "pl                                             0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inst.sort_values(['nb_poster','nb_oral','nb_workshop'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inst['acceptance_rate'] =  (df_inst.nb_poster + df_inst.nb_oral) / df_inst.nb_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_note</th>\n",
       "      <th>avg_oral</th>\n",
       "      <th>avg_poster</th>\n",
       "      <th>avg_reject</th>\n",
       "      <th>avg_workshop</th>\n",
       "      <th>nb_oral</th>\n",
       "      <th>nb_paper</th>\n",
       "      <th>nb_poster</th>\n",
       "      <th>nb_reject</th>\n",
       "      <th>nb_workshop</th>\n",
       "      <th>acceptance_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eus</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>6.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>5.916667</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.944444</td>\n",
       "      <td>4.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>au</th>\n",
       "      <td>5.857143</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>il</th>\n",
       "      <td>6.133333</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>4.888889</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jp</th>\n",
       "      <td>5.714286</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>6.571429</td>\n",
       "      <td>4.814815</td>\n",
       "      <td>5.444444</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>5.654762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>4.904762</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com</th>\n",
       "      <td>5.512671</td>\n",
       "      <td>7.145833</td>\n",
       "      <td>6.485320</td>\n",
       "      <td>4.724359</td>\n",
       "      <td>5.310606</td>\n",
       "      <td>16</td>\n",
       "      <td>513</td>\n",
       "      <td>193</td>\n",
       "      <td>260</td>\n",
       "      <td>44</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edu</th>\n",
       "      <td>5.532885</td>\n",
       "      <td>7.238095</td>\n",
       "      <td>6.501035</td>\n",
       "      <td>4.752252</td>\n",
       "      <td>5.401361</td>\n",
       "      <td>14</td>\n",
       "      <td>446</td>\n",
       "      <td>161</td>\n",
       "      <td>222</td>\n",
       "      <td>49</td>\n",
       "      <td>0.392377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uk</th>\n",
       "      <td>5.328125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.280000</td>\n",
       "      <td>4.648649</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kr</th>\n",
       "      <td>5.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.388889</td>\n",
       "      <td>4.370370</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca</th>\n",
       "      <td>5.236111</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.604167</td>\n",
       "      <td>4.240000</td>\n",
       "      <td>5.277778</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org</th>\n",
       "      <td>5.525641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.629630</td>\n",
       "      <td>4.888889</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net</th>\n",
       "      <td>5.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>4.888889</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>io</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>5.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hk</th>\n",
       "      <td>5.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch</th>\n",
       "      <td>5.309524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.370370</td>\n",
       "      <td>4.705882</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.321429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>5.368421</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>4.782609</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>5.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.777778</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dk</th>\n",
       "      <td>4.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sg</th>\n",
       "      <td>5.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>4.866667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>5.296296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cn</th>\n",
       "      <td>5.180000</td>\n",
       "      <td>6.555556</td>\n",
       "      <td>6.523810</td>\n",
       "      <td>4.644444</td>\n",
       "      <td>5.433333</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gov</th>\n",
       "      <td>3.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sa</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gr</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eu</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nz</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eg</th>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>se</th>\n",
       "      <td>5.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mil</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tw</th>\n",
       "      <td>5.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc/2018/Conference/Paper239/Authors</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>br</th>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     avg_note  avg_oral  avg_poster  \\\n",
       "eus                                  6.000000  0.000000    6.000000   \n",
       "me                                   6.000000  0.000000    6.000000   \n",
       "cc                                   5.666667  0.000000    5.666667   \n",
       "pt                                   6.222222  0.000000    6.333333   \n",
       "nl                                   5.916667  8.000000    6.944444   \n",
       "au                                   5.857143  7.000000    6.333333   \n",
       "ru                                   5.666667  0.000000    6.333333   \n",
       "il                                   6.133333  8.000000    6.833333   \n",
       "jp                                   5.714286  7.166667    6.571429   \n",
       "fr                                   5.654762  0.000000    6.750000   \n",
       "com                                  5.512671  7.145833    6.485320   \n",
       "at                                   5.200000  0.000000    6.333333   \n",
       "edu                                  5.532885  7.238095    6.501035   \n",
       "uk                                   5.328125  0.000000    6.280000   \n",
       "kr                                   5.187500  0.000000    6.388889   \n",
       "ca                                   5.236111  8.000000    6.604167   \n",
       "org                                  5.525641  0.000000    6.629630   \n",
       "net                                  5.444444  0.000000    6.666667   \n",
       "it                                   5.666667  0.000000    6.500000   \n",
       "io                                   5.000000  0.000000    6.333333   \n",
       "es                                   5.055556  0.000000    6.166667   \n",
       "hk                                   5.222222  0.000000    6.666667   \n",
       "ch                                   5.309524  0.000000    6.370370   \n",
       "de                                   5.368421  8.000000    6.500000   \n",
       "in                                   5.166667  0.000000    6.777778   \n",
       "dk                                   4.416667  0.000000    5.666667   \n",
       "sg                                   5.185185  0.000000    6.166667   \n",
       "ai                                   5.296296  0.000000    7.166667   \n",
       "cn                                   5.180000  6.555556    6.523810   \n",
       "gov                                  3.777778  0.000000    0.000000   \n",
       "sa                                   5.666667  0.000000    0.000000   \n",
       "gr                                   4.333333  0.000000    0.000000   \n",
       "name                                 4.333333  0.000000    0.000000   \n",
       "no                                   3.000000  0.000000    0.000000   \n",
       "eu                                   4.666667  0.000000    0.000000   \n",
       "nz                                   4.000000  0.000000    0.000000   \n",
       "cat                                  5.333333  0.000000    0.000000   \n",
       "eg                                   3.666667  0.000000    0.000000   \n",
       "be                                   5.333333  0.000000    0.000000   \n",
       "tr                                   4.333333  0.000000    0.000000   \n",
       "one                                  5.000000  0.000000    0.000000   \n",
       "se                                   5.166667  0.000000    0.000000   \n",
       "mil                                  4.000000  0.000000    0.000000   \n",
       "tw                                   5.037037  0.000000    0.000000   \n",
       "cc/2018/Conference/Paper239/Authors  4.666667  0.000000    0.000000   \n",
       "br                                   3.666667  0.000000    0.000000   \n",
       "pl                                   3.000000  0.000000    0.000000   \n",
       "\n",
       "                                     avg_reject  avg_workshop  nb_oral  \\\n",
       "eus                                    0.000000      0.000000        0   \n",
       "me                                     0.000000      0.000000        0   \n",
       "cc                                     0.000000      0.000000        0   \n",
       "pt                                     0.000000      6.000000        0   \n",
       "nl                                     4.266667      0.000000        1   \n",
       "au                                     5.000000      0.000000        1   \n",
       "ru                                     5.000000      0.000000        0   \n",
       "il                                     4.888889      5.666667        1   \n",
       "jp                                     4.814815      5.444444        2   \n",
       "fr                                     4.904762      4.333333        0   \n",
       "com                                    4.724359      5.310606       16   \n",
       "at                                     4.444444      0.000000        0   \n",
       "edu                                    4.752252      5.401361       14   \n",
       "uk                                     4.648649      6.000000        0   \n",
       "kr                                     4.370370      5.333333        0   \n",
       "ca                                     4.240000      5.277778        1   \n",
       "org                                    4.888889      5.333333        0   \n",
       "net                                    4.833333      0.000000        0   \n",
       "it                                     4.888889      6.333333        0   \n",
       "io                                     4.000000      4.666667        0   \n",
       "es                                     4.500000      0.000000        0   \n",
       "hk                                     4.500000      0.000000        0   \n",
       "ch                                     4.705882      5.666667        0   \n",
       "de                                     4.782609      5.250000        1   \n",
       "in                                     4.333333      5.666667        0   \n",
       "dk                                     4.000000      0.000000        0   \n",
       "sg                                     4.866667      5.000000        0   \n",
       "ai                                     4.083333      5.666667        0   \n",
       "cn                                     4.644444      5.433333        3   \n",
       "gov                                    3.777778      0.000000        0   \n",
       "sa                                     5.666667      0.000000        0   \n",
       "gr                                     4.333333      0.000000        0   \n",
       "name                                   4.333333      0.000000        0   \n",
       "no                                     3.000000      0.000000        0   \n",
       "eu                                     4.666667      0.000000        0   \n",
       "nz                                     4.000000      0.000000        0   \n",
       "cat                                    5.333333      0.000000        0   \n",
       "eg                                     3.666667      0.000000        0   \n",
       "be                                     5.333333      0.000000        0   \n",
       "tr                                     4.333333      0.000000        0   \n",
       "one                                    5.000000      0.000000        0   \n",
       "se                                     5.166667      0.000000        0   \n",
       "mil                                    4.000000      0.000000        0   \n",
       "tw                                     5.037037      0.000000        0   \n",
       "cc/2018/Conference/Paper239/Authors    4.666667      0.000000        0   \n",
       "br                                     3.222222      5.000000        0   \n",
       "pl                                     3.000000      0.000000        0   \n",
       "\n",
       "                                     nb_paper  nb_poster  nb_reject  \\\n",
       "eus                                         1          1          0   \n",
       "me                                          1          1          0   \n",
       "cc                                          1          1          0   \n",
       "pt                                          3          2          0   \n",
       "nl                                         12          6          5   \n",
       "au                                          7          3          3   \n",
       "ru                                          4          2          2   \n",
       "il                                         10          4          3   \n",
       "jp                                         21          7          9   \n",
       "fr                                         28         12         14   \n",
       "com                                       513        193        260   \n",
       "at                                          5          2          3   \n",
       "edu                                       446        161        222   \n",
       "uk                                         64         25         37   \n",
       "kr                                         16          6          9   \n",
       "ca                                         48         16         25   \n",
       "org                                        26          9         15   \n",
       "net                                         3          1          2   \n",
       "it                                          6          2          3   \n",
       "io                                          3          1          1   \n",
       "es                                          6          2          4   \n",
       "hk                                          6          2          4   \n",
       "ch                                         28          9         17   \n",
       "de                                         38         10         23   \n",
       "in                                         12          3          7   \n",
       "dk                                          4          1          3   \n",
       "sg                                          9          2          5   \n",
       "ai                                          9          2          4   \n",
       "cn                                         50          7         30   \n",
       "gov                                         3          0          3   \n",
       "sa                                          1          0          1   \n",
       "gr                                          2          0          2   \n",
       "name                                        1          0          1   \n",
       "no                                          1          0          1   \n",
       "eu                                          2          0          2   \n",
       "nz                                          1          0          1   \n",
       "cat                                         1          0          1   \n",
       "eg                                          1          0          1   \n",
       "be                                          1          0          1   \n",
       "tr                                          3          0          3   \n",
       "one                                         1          0          1   \n",
       "se                                          2          0          2   \n",
       "mil                                         1          0          1   \n",
       "tw                                          9          0          9   \n",
       "cc/2018/Conference/Paper239/Authors         1          0          1   \n",
       "br                                          4          0          3   \n",
       "pl                                          1          0          1   \n",
       "\n",
       "                                     nb_workshop  acceptance_rate  \n",
       "eus                                            0         1.000000  \n",
       "me                                             0         1.000000  \n",
       "cc                                             0         1.000000  \n",
       "pt                                             1         0.666667  \n",
       "nl                                             0         0.583333  \n",
       "au                                             0         0.571429  \n",
       "ru                                             0         0.500000  \n",
       "il                                             2         0.500000  \n",
       "jp                                             3         0.428571  \n",
       "fr                                             2         0.428571  \n",
       "com                                           44         0.407407  \n",
       "at                                             0         0.400000  \n",
       "edu                                           49         0.392377  \n",
       "uk                                             2         0.390625  \n",
       "kr                                             1         0.375000  \n",
       "ca                                             6         0.354167  \n",
       "org                                            2         0.346154  \n",
       "net                                            0         0.333333  \n",
       "it                                             1         0.333333  \n",
       "io                                             1         0.333333  \n",
       "es                                             0         0.333333  \n",
       "hk                                             0         0.333333  \n",
       "ch                                             2         0.321429  \n",
       "de                                             4         0.289474  \n",
       "in                                             2         0.250000  \n",
       "dk                                             0         0.250000  \n",
       "sg                                             2         0.222222  \n",
       "ai                                             3         0.222222  \n",
       "cn                                            10         0.200000  \n",
       "gov                                            0         0.000000  \n",
       "sa                                             0         0.000000  \n",
       "gr                                             0         0.000000  \n",
       "name                                           0         0.000000  \n",
       "no                                             0         0.000000  \n",
       "eu                                             0         0.000000  \n",
       "nz                                             0         0.000000  \n",
       "cat                                            0         0.000000  \n",
       "eg                                             0         0.000000  \n",
       "be                                             0         0.000000  \n",
       "tr                                             0         0.000000  \n",
       "one                                            0         0.000000  \n",
       "se                                             0         0.000000  \n",
       "mil                                            0         0.000000  \n",
       "tw                                             0         0.000000  \n",
       "cc/2018/Conference/Paper239/Authors            0         0.000000  \n",
       "br                                             1         0.000000  \n",
       "pl                                             0         0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inst.sort_values(['acceptance_rate'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_inst.to_json('country_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
